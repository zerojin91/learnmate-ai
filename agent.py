"""
MCP Agent ëª¨ë“ˆ - Stateful Multi-Agent Systemê³¼ ì—°ë™
"""

from typing import AsyncGenerator, Optional, List, Dict
import json
import re
from pydantic import BaseModel, Field
from enum import Enum

from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI
from utils import astream_graph, trim_conversation_history, log_token_usage, apply_chat_template
from config import Config

class ActionType(str, Enum):
    """ì‚¬ìš©ì ë©”ì‹œì§€ì— ëŒ€í•œ ì•¡ì…˜ ìœ í˜•"""
    GENERAL_CHAT = "general_chat"           # ì¼ë°˜ ëŒ€í™”
    USER_PROFILING = "user_profiling"       # í•™ìŠµ í”„ë¡œí•„ ìˆ˜ì§‘ í•„ìš”
    GENERATE_CURRICULUM = "generate_curriculum"  # ì»¤ë¦¬í˜ëŸ¼ ìƒì„±
    MENTOR_RECOMMENDATION = "mentor_recommendation"  # ì „ë¬¸ê°€ ë©˜í†  í˜ë¥´ì†Œë‚˜ ì¶”ì²œ
    MENTOR_CHAT = "mentor_chat"  # ì „ë¬¸ê°€ ë©˜í† ë§ ëŒ€í™”
    PROFILING_GENERAL_CHAT = "profiling_general_chat"  # í”„ë¡œíŒŒì¼ë§ ì¤‘ ì¼ë°˜ ëŒ€í™”

class ActionClassification(BaseModel):
    """ì•¡ì…˜ ë¶„ë¥˜ ê²°ê³¼"""
    action: ActionType = Field(description="ìˆ˜í–‰í•  ì•¡ì…˜ íƒ€ì…")
    
class MultiMCPAgent:
    """ì—¬ëŸ¬ MCP ì„œë²„ë¥¼ ë™ì‹œì— ì—°ê²°í•˜ëŠ” ì—ì´ì „íŠ¸ with Stateful Assessment"""
    
    def __init__(self, server_scripts: List[str]):
        self.server_scripts = server_scripts
        self.client = None
        self.agent = None
        self.initialized = False
        
        # ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬
        self.current_session_id = None
        
        # ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ - ê¸°ë³¸ ì¸ì‚¬ë§ í¬í•¨
        self.conversation_history: List[Dict[str, str]] = [
            {
                "role": "assistant", 
                "content": "ì•ˆë…•í•˜ì„¸ìš”! LearnAI ì…ë‹ˆë‹¤. ì–´ë–¤ ì£¼ì œì— ëŒ€í•´ ë°°ìš°ê³  ì‹¶ìœ¼ì‹ ì§€ ì•Œë ¤ì£¼ì‹œë©´ ë§ì¶¤í˜• í•™ìŠµ ê³„íšì„ í•¨ê»˜ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤!"
            }
        ]
        self.max_tokens = Config.LLM_MAX_TOKENS

        # LLM ì„¤ì •
        self.llm = ChatOpenAI(
            base_url=Config.LLM_BASE_URL,
            api_key=Config.LLM_API_KEY,
            model=Config.LLM_MODEL,
            temperature=Config.LLM_TEMPERATURE,
            max_tokens=self.max_tokens,
            model_kwargs={"max_completion_tokens": None}  # Friendli.aiì—ì„œ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„° ì œê±°
        )
         
    async def initialize(self):
        """ì—¬ëŸ¬ MCP ì„œë²„ ë™ì‹œ ì´ˆê¸°í™” - ê³µì‹ ë°©ë²• ì‚¬ìš©"""
        if self.initialized:
            return
            
        try:
            # MCP ì„œë²„ ì„¤ì •ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            server_configs = {}
            for i, server_script in enumerate(self.server_scripts):
                server_name = f"server_{i}"
                server_configs[server_name] = {
                    "command": "python",
                    "args": [server_script],
                    "transport": "stdio"
                }
            
            print(f"ì„œë²„ ì„¤ì •: {server_configs}")
            
            # MultiServerMCPClient ìƒì„±
            self.client = MultiServerMCPClient(server_configs)
            
            # ëª¨ë“  ë„êµ¬ ê°€ì ¸ì˜¤ê¸°
            tools = await self.client.get_tools()
            print(f"ë¡œë“œëœ ë„êµ¬ë“¤: {[tool.name for tool in tools]}")
            
            # ReAct ì—ì´ì „íŠ¸ ìƒì„±
            self.agent = create_react_agent(self.llm, tools)
            
            self.initialized = True
            print(f"âœ… MultiMCP Agent ì´ˆê¸°í™” ì™„ë£Œ - {len(tools)}ê°œ ë„êµ¬ ë¡œë“œë¨!")
            
        except Exception as e:
            print(f"âŒ MultiMCP Agent ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            await self.cleanup()
            raise e
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self.client:
                # MultiServerMCPClientëŠ” close ë©”ì„œë“œê°€ ì—†ìœ¼ë¯€ë¡œ ì œê±°
                pass
            self.initialized = False
        except Exception as e:
            print(f"Cleanup error: {e}")
    
    def _extract_session_id(self, response_content: str) -> Optional[str]:
        """ì‘ë‹µì—ì„œ ì„¸ì…˜ ID ì¶”ì¶œ"""
        session_match = re.search(r'Session:\s*([a-zA-Z0-9-]+)', response_content)
        if session_match:
            return session_match.group(1)
        return None
    
    
    async def chat(self, message: str) -> AsyncGenerator[dict, None]:
        """ë©€í‹°í„´ ëŒ€í™” ì²˜ë¦¬ - Stateful Assessment ì§€ì›"""
        if not self.initialized:
            await self.initialize()
        
        try:
            print(f"ğŸ“ ì‚¬ìš©ì ë©”ì‹œì§€: {message}")
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€ (ì›ë³¸ ì €ì¥)
            self.conversation_history.append({"role": "user", "content": message})
            
            # í† í° ì œí•œì— ë§ê²Œ ëŒ€í™” ê¸°ë¡ ì •ë¦¬
            self.conversation_history = trim_conversation_history(self.conversation_history, self.max_tokens)
            
            # í† í° ì‚¬ìš©ëŸ‰ ë¡œê·¸
            log_token_usage(self.conversation_history)
            
            # MCP ë°©ì‹: ReAct ì—ì´ì „íŠ¸ê°€ í•„ìš”í•œ ë„êµ¬ë¥¼ ìë™ìœ¼ë¡œ ì„ íƒ
            async for chunk in self._handle_unified_conversation(message):
                yield chunk
                        
        except Exception as e:
            yield {
                "type": "error",
                "content": f"ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }
    
    
    async def _classify_user_intent(self, message: str) -> ActionClassification:
        """ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë¶„ë¥˜í•˜ì—¬ ì ì ˆí•œ ì•¡ì…˜ ê²°ì • - í”„ë¡œíŒŒì¼ë§ ìš°ì„ ìˆœìœ„ ê¸°ë°˜"""

        # í”„ë¡œíŒŒì¼ë§ ìƒíƒœ í™•ì¸ (ìµœìš°ì„ )
        profiling_status = await self._get_profiling_status()

        # ğŸ¯ í”„ë¡œíŒŒì¼ë§ ì§„í–‰ ì¤‘ì´ë©´ ë‹¨ìˆœ 2ë¶„ë²•
        if profiling_status["in_progress"]:
            print(f"ğŸ“Š í”„ë¡œíŒŒì¼ë§ ì§„í–‰ ì¤‘ - 2ë¶„ë²• ë¶„ë¥˜ ì ìš©")

            # General Chat ì—¬ë¶€ë§Œ íŒë‹¨
            general_chat_prompt = f"""ë‹¤ìŒ ë©”ì‹œì§€ê°€ í•™ìŠµê³¼ ì™„ì „íˆ ë¬´ê´€í•œ ì¼ìƒ ëŒ€í™”ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”:

ë©”ì‹œì§€: "{message}"

**í•™ìŠµ ê´€ë ¨ìœ¼ë¡œ ë³´ëŠ” ê²½ìš°:**
- ì‹œê°„ ì–¸ê¸‰ : "ì£¼ 3ì‹œê°„", "ì£¼ 1ì‹œê°„", "í•˜ë£¨ 1ì‹œê°„"
- ìˆ˜ì¤€ ì–¸ê¸‰: "ì™„ì „ ì´ˆë³´ì", "ì–´ë ¤ì›Œ", "ì¤‘ê¸‰ì", "ê³ ê¸‰ì", "ì´ˆë³´", "ì²˜ìŒ"
- ëª©í‘œ ì–¸ê¸‰: "ì·¨ì—…", "ì´ì§", "í”„ë¡œì íŠ¸", "ì·¨ì—… ì¤€ë¹„", "ì´ì§ ì¤€ë¹„", "í”„ë¡œì íŠ¸ ì¤€ë¹„"
- í•™ìŠµ ì˜ì§€: "ë°°ìš°ê³  ì‹¶ì–´", "ê³µë¶€í•˜ê³  ì‹¶ì–´", "í•™ìŠµí•˜ê³  ì‹¶ì–´"
- ê¸°íƒ€ í•™ìŠµ ê´€ë ¨ ëª¨ë“  ë‚´ìš©

**íŒë‹¨ ê¸°ì¤€**: í•™ìŠµê³¼ 100% ë¬´ê´€í•˜ê³  ëª…ë°±í•œ ì¼ìƒ ëŒ€í™”ë§Œ general_chatìœ¼ë¡œ ë¶„ë¥˜
ì• ë§¤í•˜ë©´ ë¬´ì¡°ê±´ í•™ìŠµ ê´€ë ¨ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”. íŠ¹íˆ 'ì‹œê°„', 'ìˆ˜ì¤€', 'ëª©í‘œ', 'í•™ìŠµ ì˜ì§€' ì™€ ê´€ë ¨ëœ ë©”ì‹œì§€ëŠ” ë¬´ì¡°ê±´ í•™ìŠµ ê´€ë ¨ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”."""

            try:
                # General Chat íŒë‹¨ì„ ìœ„í•œ ë³„ë„ ë¶„ë¥˜
                from pydantic import BaseModel, Field

                class GeneralChatCheck(BaseModel):
                    is_profiling_chat: bool = Field(description="í•™ìŠµ ê´€ë ¨ ëŒ€í™” ì—¬ë¶€")

                checker_model = self.llm.with_structured_output(GeneralChatCheck)
                check_result = checker_model.invoke(general_chat_prompt)

                if check_result.is_profiling_chat:
                    print(f"ğŸ” í”„ë¡œíŒŒì¼ë§ ì¤‘ í•™ìŠµ ê´€ë ¨ìœ¼ë¡œ ë¶„ë¥˜")
                    return ActionClassification(action=ActionType.USER_PROFILING)
                else:
                    print(f"ğŸ” í”„ë¡œíŒŒì¼ë§ ì¤‘ ì¼ë°˜ ëŒ€í™”ë¡œ ë¶„ë¥˜")
                    return ActionClassification(action=ActionType.PROFILING_GENERAL_CHAT)

            except Exception as e:
                print(f"âŒ ì¼ë°˜ ëŒ€í™” ì²´í¬ ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ ì‹œ ì•ˆì „í•˜ê²Œ í”„ë¡œíŒŒì¼ë§ìœ¼ë¡œ
                return ActionClassification(action=ActionType.USER_PROFILING)

        # ğŸ¯ í”„ë¡œíŒŒì¼ë§ ì§„í–‰ ì¤‘ì´ ì•„ë‹ ë•ŒëŠ” ê¸°ì¡´ ë¡œì§
        mentor_session_phase = await self._check_mentor_session_status()

        classification_prompt = f"""ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë‹¤ìŒ 3ê°€ì§€ ì•¡ì…˜ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:

1. **general_chat**: ì¼ë°˜ì ì¸ ì¸ì‚¬, ì•ˆë¶€, ê°ì‚¬ ë“± í•™ìŠµê³¼ ë¬´ê´€í•œ ëŒ€í™”
2. **user_profiling**: í•™ìŠµ ê´€ë ¨ ìš”ì²­ì´ì§€ë§Œ ì‚¬ìš©ì í”„ë¡œí•„ì´ í•„ìš”í•œ ê²½ìš°
3. **generate_curriculum**: ì´ë¯¸ í•™ìŠµ í”„ë¡œí•„ì´ ìˆê³  ì»¤ë¦¬í˜ëŸ¼/ê³„íš ìƒì„±ì„ ìš”ì²­í•˜ëŠ” ê²½ìš°

ì‚¬ìš©ì ë©”ì‹œì§€: "{message}"
í˜„ì¬ ë©˜í†  ì„¸ì…˜ ìƒíƒœ: {mentor_session_phase}

## ë¶„ë¥˜ ê¸°ì¤€:
- "ì•ˆë…•", "ê³ ë§ˆì›Œ", "ì˜ê°€", "ë¼ë©´ë¨¹ê³ ì‹¶ì–´" ë“± â†’ general_chat
- "~ë°°ìš°ê³  ì‹¶ì–´", "~ê³µë¶€í•˜ê³  ì‹¶ì–´", "~ê°€ë¥´ì³ì¤˜" ë“± â†’ user_profiling
- "ì»¤ë¦¬í˜ëŸ¼ ë§Œë“¤ì–´ì¤˜", "í•™ìŠµê³„íš ì„¸ì›Œì¤˜", "ë¡œë“œë§µ ë³´ì—¬ì¤˜" ë“± â†’ generate_curriculum

ì •í™•í•œ ì•¡ì…˜ë§Œ ì„ íƒí•˜ì„¸ìš”."""

        try:
            classifier_model = self.llm.with_structured_output(ActionClassification)
            result = classifier_model.invoke(classification_prompt)
            print(f"ğŸ” ì˜ë„ ë¶„ë¥˜ ê²°ê³¼: {result.action}")
            return result
        except Exception as e:
            print(f"âŒ ì˜ë„ ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ê°’ìœ¼ë¡œ ì¼ë°˜ ëŒ€í™” ì„ íƒ
            return ActionClassification(action=ActionType.GENERAL_CHAT)

    async def _handle_user_profiling(self, message: str) -> AsyncGenerator[dict, None]:
        """user_profiling ë„êµ¬ë¥¼ ì‚¬ìš©í•œ í”„ë¡œí•„ ìˆ˜ì§‘"""
        print(f"ğŸ“Š ì‚¬ìš©ì í”„ë¡œí•„ë§ ì‹œì‘")
        
        try:
            # user_profiling ë„êµ¬ ì°¾ê¸°
            tools = await self.client.get_tools()
            user_profiling_tool = next((tool for tool in tools if tool.name == "user_profiling"), None)
            
            if not user_profiling_tool:
                yield {"type": "error", "content": "ì‚¬ìš©ì í”„ë¡œí•„ë§ ë„êµ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
                return
            
            # ë„êµ¬ ì‹¤í–‰
            tool_args = {"user_message": message, "session_id": self.current_session_id}
            print(f"ğŸ”§ user_profiling í˜¸ì¶œ: {tool_args}")
            
            result = await user_profiling_tool.ainvoke(tool_args)
            
            if result:
                print(result, end="", flush=True)
                self.conversation_history.append({"role": "assistant", "content": result})
                
                # ë„êµ¬ í˜¸ì¶œ í›„ ìµœì‹  í”„ë¡œí•„ ì •ë³´ ë¡œë“œ
                profile_data = None
                try:
                    from servers.user_assessment import load_session
                    if self.current_session_id:
                        session_data = load_session(self.current_session_id)
                        if session_data:
                            profile_info = {
                                'topic': session_data.get('topic', ''),
                                'constraints': session_data.get('constraints', ''),
                                'goal': session_data.get('goal', '')
                            }
                            profile_data = {k: v for k, v in profile_info.items() if v}
                            print(f"ğŸ“Š ìµœì‹  í”„ë¡œí•„ ë¡œë“œ: {profile_data}")
                except Exception as e:
                    print(f"í”„ë¡œí•„ ë¡œë“œ ì˜¤ë¥˜: {e}")
                
                response_data = {"type": "message", "content": result, "node": "user_profiling"}
                if profile_data:
                    response_data["profile"] = profile_data
                    
                yield response_data
                
        except Exception as e:
            print(f"âŒ ì‚¬ìš©ì í”„ë¡œí•„ë§ ì˜¤ë¥˜: {e}")
            yield {"type": "error", "content": f"í”„ë¡œí•„ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}

    async def _handle_curriculum_generation(self, message: str) -> AsyncGenerator[dict, None]:
        """generate_curriculum_from_session ë„êµ¬ë¥¼ ì‚¬ìš©í•œ ì»¤ë¦¬í˜ëŸ¼ ìƒì„±"""
        print(f"ğŸ“š ì»¤ë¦¬í˜ëŸ¼ ìƒì„± ì‹œì‘")
        
        try:
            # generate_curriculum_from_session ë„êµ¬ ì°¾ê¸°
            tools = await self.client.get_tools()
            curriculum_tool = next((tool for tool in tools if tool.name == "generate_curriculum_from_session"), None)
            
            if not curriculum_tool:
                yield {"type": "error", "content": "ì»¤ë¦¬í˜ëŸ¼ ìƒì„± ë„êµ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
                return
            
            # ë„êµ¬ ì‹¤í–‰ (ì‚¬ìš©ì ë©”ì‹œì§€ë„ ì „ë‹¬)
            tool_args = {
                "session_id": self.current_session_id,
                "user_message": message
            }
            print(f"ğŸ”§ generate_curriculum_from_session í˜¸ì¶œ: {tool_args}")
            
            result = await curriculum_tool.ainvoke(tool_args)
            
            if result:
                print(result, end="", flush=True)
                self.conversation_history.append({"role": "assistant", "content": result})
                yield {"type": "message", "content": result, "node": "generate_curriculum"}
                
        except Exception as e:
            print(f"âŒ ì»¤ë¦¬í˜ëŸ¼ ìƒì„± ì˜¤ë¥˜: {e}")
            yield {"type": "error", "content": f"ì»¤ë¦¬í˜ëŸ¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}

    async def _handle_general_chat(self, message: str) -> AsyncGenerator[dict, None]:
        """ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬ (ë„êµ¬ ì—†ì´)"""
        print(f"ğŸ’¬ ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬")
        
        try:
            # LearnAI ì„±ê²©ì˜ ì¼ë°˜ ëŒ€í™” í”„ë¡¬í”„íŠ¸
            system_prompt = """ë‹¹ì‹ ì€ LearnAIì˜ ì¹œê·¼í•œ í•™ìŠµ ë©˜í† ì…ë‹ˆë‹¤.
            
ë”°ëœ»í•˜ê³  ê²©ë ¤í•˜ëŠ” ì„±ê²©ìœ¼ë¡œ ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”.
ì¼ë°˜ì ì¸ ì¸ì‚¬, ì•ˆë¶€, ê°ì‚¬ ë“±ì— ì¹œê·¼í•˜ê²Œ ì‘ë‹µí•˜ë˜, 
í•­ìƒ í•™ìŠµì— ëŒ€í•œ ê´€ì‹¬ì„ ì—´ì–´ë‘ê³  ë„ì›€ì´ í•„ìš”í•˜ë©´ ì–¸ì œë“  ë§í•´ë‹¬ë¼ê³  ê²©ë ¤í•˜ì„¸ìš”."""

            from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
            
            messages = [SystemMessage(content=system_prompt)]
            
            # ìµœê·¼ ëŒ€í™” ê¸°ë¡ë§Œ í¬í•¨ (í† í° ì ˆì•½)
            for item in self.conversation_history[-4:]:
                if item["role"] == "user":
                    messages.append(HumanMessage(content=item["content"]))
                elif item["role"] == "assistant":
                    messages.append(AIMessage(content=item["content"]))
            
            # LLM ì§ì ‘ í˜¸ì¶œ (ë„êµ¬ ì—†ì´)
            response_content = ""
            async for chunk in self.llm.astream(messages):
                if hasattr(chunk, 'content') and chunk.content:
                    response_content += chunk.content
                    print(chunk.content, end="", flush=True)
                    yield {"type": "message", "content": chunk.content, "node": "general_chat"}
            
            if response_content:
                self.conversation_history.append({"role": "assistant", "content": response_content})
                
        except Exception as e:
            print(f"âŒ ì¼ë°˜ ëŒ€í™” ì˜¤ë¥˜: {e}")
            yield {"type": "error", "content": f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}

    async def _handle_profiling_general_chat_sequential(self, message: str) -> AsyncGenerator[dict, None]:
        """í”„ë¡œíŒŒì¼ë§ ì¤‘ general chat ìˆœì°¨ ì²˜ë¦¬: ì¼ë°˜ ì‘ë‹µ â†’ ì—°ê²° â†’ í”„ë¡œíŒŒì¼ë§"""
        print(f"ğŸ”„ í”„ë¡œíŒŒì¼ë§ ì¤‘ ì¼ë°˜ ëŒ€í™” ìˆœì°¨ ì²˜ë¦¬")

        try:
            # 1ë‹¨ê³„: í”„ë¡œíŒŒì¼ë§ì´ ì§„í–‰ ì¤‘ì¸ì§€ í™•ì¸
            profiling_status = await self._get_profiling_status()

            if not profiling_status["in_progress"]:
                # í”„ë¡œíŒŒì¼ë§ì´ ì§„í–‰ì¤‘ì´ ì•„ë‹ˆë¼ë©´ ì¼ë°˜ ëŒ€í™”ë¡œ ì²˜ë¦¬
                async for chunk in self._handle_general_chat(message):
                    yield chunk
                return

            # 2ë‹¨ê³„: í”„ë¡œíŒŒì¼ë§ ìƒíƒœ í™•ì¸
            profiling_status = await self._get_profiling_status()
            missing_info = []
            if not profiling_status.get("topic"):
                missing_info.append("í•™ìŠµ ì£¼ì œ")
            if not profiling_status.get("constraints"):
                missing_info.append("í˜„ì¬ ìˆ˜ì¤€ê³¼ ì‹œê°„")
            if not profiling_status.get("goal"):
                missing_info.append("í•™ìŠµ ëª©í‘œ")

            # 3ë‹¨ê³„: ìì—°ìŠ¤ëŸ¬ìš´ í†µí•© ì‘ë‹µ ìƒì„±
            integrated_prompt = f"""ì‚¬ìš©ìê°€ ì¼ë°˜ì ì¸ ëŒ€í™”ë¥¼ í–ˆìŠµë‹ˆë‹¤: "{message}"

ì´ì— ëŒ€í•´ ì¹œê·¼í•˜ê²Œ ì‘ë‹µí•œ í›„, ìì—°ìŠ¤ëŸ½ê²Œ í•™ìŠµ í”„ë¡œíŒŒì¼ë§ìœ¼ë¡œ ì—°ê²°í•´ì£¼ì„¸ìš”.

í˜„ì¬ ì•„ì§ íŒŒì•…í•˜ì§€ ëª»í•œ ì •ë³´: {', '.join(missing_info) if missing_info else 'ì—†ìŒ'}

ìš”êµ¬ì‚¬í•­:
1. ë¨¼ì € ì‚¬ìš©ìì˜ ë§ì— ê³µê°í•˜ê³  ì¹œê·¼í•˜ê²Œ ë°˜ì‘
2. ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°ì–´ë‚˜ ë¬¸ì¥ìœ¼ë¡œ í•™ìŠµ ê´€ë ¨ ì§ˆë¬¸ìœ¼ë¡œ ì´ì–´ê°€ê¸°
3. "ê·¸ëŸ°ë°"ì™€ ê°™ì€ ì–´ìƒ‰í•œ ì—°ê²°ì–´ í”¼í•˜ê¸°
4. ì „ì²´ ì‘ë‹µì´ í•˜ë‚˜ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ì²˜ëŸ¼ ëŠê»´ì§€ë„ë¡

ì˜ˆì‹œ:
ì‚¬ìš©ì: "í”¼ê³¤í•´"
ì‘ë‹µ: "ìˆ˜ê³  ë§ìœ¼ì…¨ì–´ìš”! ğŸ˜Š ì˜¤ëŠ˜ í•˜ë£¨ ì •ë§ ê³ ìƒí•˜ì…¨ë„¤ìš”. íœ´ì‹ë„ ì¤‘ìš”í•˜ì§€ë§Œ, í˜¹ì‹œ ì–´ë–¤ ë¶„ì•¼ë¥¼ ë°°ìš°ê³  ì‹¶ìœ¼ì‹ ì§€ ê¶ê¸ˆí•´ìš”!"

ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ í•˜ë‚˜ì˜ ì™„ì „í•œ ì‘ë‹µì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”."""

            from langchain_core.messages import HumanMessage, SystemMessage

            messages = [
                SystemMessage(content="ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í•™ìŠµ ë©˜í† ì…ë‹ˆë‹¤. ì¼ë°˜ ëŒ€í™”ì™€ í•™ìŠµ ì§ˆë¬¸ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•˜ì„¸ìš”."),
                HumanMessage(content=integrated_prompt)
            ]

            # í†µí•© ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
            integrated_response = ""
            async for chunk in self.llm.astream(messages):
                if hasattr(chunk, 'content') and chunk.content:
                    integrated_response += chunk.content
                    print(chunk.content, end="", flush=True)
                    yield {"type": "message", "content": chunk.content, "node": "integrated_chat"}

            # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
            if integrated_response:
                self.conversation_history.append({
                    "role": "assistant",
                    "content": integrated_response.strip()
                })

        except Exception as e:
            print(f"âŒ ìˆœì°¨ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            yield {"type": "error", "content": f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}

    async def _get_profiling_status(self) -> dict:
        """í˜„ì¬ í”„ë¡œíŒŒì¼ë§ ìƒíƒœ í™•ì¸"""
        try:
            if not self.current_session_id:
                return {"in_progress": False, "missing_step": None, "completion_rate": 0}

            from servers.user_assessment import load_session
            session_data = load_session(self.current_session_id)

            if not session_data:
                return {"in_progress": False, "missing_step": None, "completion_rate": 0}

            topic = session_data.get("topic", "").strip()
            constraints = session_data.get("constraints", "").strip()
            goal = session_data.get("goal", "").strip()

            topic_complete = bool(topic)
            # ì œì•½ì¡°ê±´ì€ ìˆ˜ì¤€ê³¼ ì‹œê°„ì´ ëª¨ë‘ ìˆì–´ì•¼ ì™„ë£Œë¡œ ê°„ì£¼ (ì‰¼í‘œë¡œ êµ¬ë¶„)
            constraints_complete = bool(constraints and "," in constraints)
            goal_complete = bool(goal)

            completed_steps = []
            if topic_complete: completed_steps.append("topic")
            if constraints_complete: completed_steps.append("constraints")
            if goal_complete: completed_steps.append("goal")

            completion_rate = len(completed_steps) / 3.0

            # ë‹¤ìŒ í•„ìš”í•œ ë‹¨ê³„ ê²°ì •
            missing_step = None
            if not topic_complete:
                missing_step = "topic"
            elif not constraints_complete:
                missing_step = "constraints"
            elif not goal_complete:
                missing_step = "goal"

            is_in_progress = not (topic_complete and constraints_complete and goal_complete)

            return {
                "in_progress": is_in_progress,
                "missing_step": missing_step,
                "completion_rate": completion_rate,
                "completed_steps": completed_steps,
                "topic": topic,
                "constraints": constraints,
                "goal": goal
            }

        except Exception as e:
            print(f"âŒ í”„ë¡œíŒŒì¼ë§ ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {e}")
            return {"in_progress": False, "missing_step": None, "completion_rate": 0}

    async def _check_mentor_session_status(self) -> str:
        """í˜„ì¬ ë©˜í†  ì„¸ì…˜ ìƒíƒœ í™•ì¸"""
        try:
            if not self.current_session_id:
                return "no_session"

            # get_mentor_session_status ë„êµ¬ ì°¾ê¸°
            tools = await self.client.get_tools()
            status_tool = next((tool for tool in tools if tool.name == "get_mentor_session_status"), None)
            
            if not status_tool:
                return "no_mentor_tool"
            
            # ë„êµ¬ ì‹¤í–‰
            result = await status_tool.ainvoke({"session_id": self.current_session_id})
            
            if isinstance(result, dict):
                if result.get("status") == "active":
                    return result.get("phase", "persona_recommendation")
            return "inactive"
            
        except Exception as e:
            print(f"âŒ ë©˜í†  ì„¸ì…˜ ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {e}")
            return "error"

    async def _handle_mentor_recommendation(self, message: str) -> AsyncGenerator[dict, None]:
        """ì „ë¬¸ê°€ ë©˜í†  í˜ë¥´ì†Œë‚˜ ì¶”ì²œ ì²˜ë¦¬"""
        print(f"ğŸ¯ ë©˜í†  í˜ë¥´ì†Œë‚˜ ì¶”ì²œ ì‹œì‘")
        
        try:
            # analyze_and_recommend_personas ë„êµ¬ ì°¾ê¸°
            tools = await self.client.get_tools()
            recommend_tool = next((tool for tool in tools if tool.name == "analyze_and_recommend_personas"), None)
            
            if not recommend_tool:
                yield {"type": "error", "content": "ë©˜í†  ì¶”ì²œ ë„êµ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
                return
            
            # ë„êµ¬ ì‹¤í–‰
            tool_args = {"message": message, "session_id": self.current_session_id}
            print(f"ğŸ”§ analyze_and_recommend_personas í˜¸ì¶œ: {tool_args}")
            
            result = await recommend_tool.ainvoke(tool_args)
            
            if result:
                print(result, end="", flush=True)
                self.conversation_history.append({"role": "assistant", "content": result})
                yield {"type": "message", "content": result, "node": "mentor_recommendation"}
                
        except Exception as e:
            print(f"âŒ ë©˜í†  ì¶”ì²œ ì˜¤ë¥˜: {e}")
            yield {"type": "error", "content": f"ë©˜í†  ì¶”ì²œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}

    async def _handle_mentor_chat(self, message: str) -> AsyncGenerator[dict, None]:
        """ì „ë¬¸ê°€ ë©˜í† ë§ ëŒ€í™” ì²˜ë¦¬"""
        print(f"ğŸ‘¨â€ğŸ« ì „ë¬¸ê°€ ë©˜í† ë§ ëŒ€í™” ì²˜ë¦¬")
        
        try:
            # í˜ë¥´ì†Œë‚˜ ì„ íƒì¸ì§€ ì¼ë°˜ ë©˜í† ë§ì¸ì§€ í™•ì¸
            if any(keyword in message.lower() for keyword in ["ì„ íƒ", "ê³ ë¥´", "ê²°ì •", "ì›í•´"]):
                # í˜ë¥´ì†Œë‚˜ ì„ íƒ ì²˜ë¦¬
                select_tool = None
                tools = await self.client.get_tools()
                select_tool = next((tool for tool in tools if tool.name == "select_persona"), None)
                
                if select_tool:
                    # ë©”ì‹œì§€ì—ì„œ í˜ë¥´ì†Œë‚˜ ID ì¶”ì¶œ (ê°„ë‹¨í•œ ë§¤í•‘)
                    persona_mapping = {
                        "ê±´ì¶•": "architecture",
                        "í† ëª©": "civil_urban", "ë„ì‹œ": "civil_urban",
                        "êµí†µ": "transport", "ìš´ì†¡": "transport",
                        "ê¸°ê³„": "mechanical", "ê¸ˆì†": "mechanical",
                        "ì „ê¸°": "electrical", "ì „ì": "electrical",
                        "ì •ë°€": "precision_energy", "ì—ë„ˆì§€": "precision_energy",
                        "ì†Œì¬": "materials", "ì¬ë£Œ": "materials",
                        "ì»´í“¨í„°": "computer", "í†µì‹ ": "computer",
                        "ì‚°ì—…": "industrial",
                        "í™”ê³µ": "chemical"
                    }
                    
                    selected_persona = None
                    for key, value in persona_mapping.items():
                        if key in message:
                            selected_persona = value
                            break
                    
                    if selected_persona:
                        tool_args = {"persona_id": selected_persona, "session_id": self.current_session_id}
                        result = await select_tool.ainvoke(tool_args)
                        
                        if result:
                            print(result, end="", flush=True)
                            self.conversation_history.append({"role": "assistant", "content": result})
                            yield {"type": "message", "content": result, "node": "mentor_selection"}
                            return
            
            # ì¼ë°˜ ë©˜í† ë§ ëŒ€í™” ì²˜ë¦¬
            expert_tool = None
            tools = await self.client.get_tools()
            expert_tool = next((tool for tool in tools if tool.name == "expert_mentoring"), None)
            
            if not expert_tool:
                yield {"type": "error", "content": "ì „ë¬¸ê°€ ë©˜í† ë§ ë„êµ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
                return
            
            # ë„êµ¬ ì‹¤í–‰
            tool_args = {"message": message, "session_id": self.current_session_id}
            print(f"ğŸ”§ expert_mentoring í˜¸ì¶œ: {tool_args}")
            
            result = await expert_tool.ainvoke(tool_args)
            
            if result:
                print(result, end="", flush=True)
                self.conversation_history.append({"role": "assistant", "content": result})
                yield {"type": "message", "content": result, "node": "mentor_chat"}
                
        except Exception as e:
            print(f"âŒ ë©˜í†  ì±„íŒ… ì˜¤ë¥˜: {e}")
            yield {"type": "error", "content": f"ë©˜í† ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}

    async def _handle_unified_conversation(self, message: str) -> AsyncGenerator[dict, None]:
        """ë¶„ë¥˜ ê¸°ë°˜ ëŒ€í™” ì²˜ë¦¬ - with_structured_outputìœ¼ë¡œ ëª…í™•í•œ ì•¡ì…˜ ì„ íƒ"""
        print(f"ğŸ¤– ë¶„ë¥˜ ê¸°ë°˜ ëŒ€í™” ì²˜ë¦¬ ì‹œì‘")
        
        try:
            # 1. ì‚¬ìš©ì ì˜ë„ ë¶„ë¥˜
            classification = await self._classify_user_intent(message)
            
            # 2. ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¼ ì²˜ë¦¬
            if classification.action == ActionType.USER_PROFILING:
                async for chunk in self._handle_user_profiling(message):
                    yield chunk
                    
            elif classification.action == ActionType.GENERATE_CURRICULUM:
                async for chunk in self._handle_curriculum_generation(message):
                    yield chunk
                    
            elif classification.action == ActionType.MENTOR_RECOMMENDATION:
                async for chunk in self._handle_mentor_recommendation(message):
                    yield chunk
                    
            elif classification.action == ActionType.MENTOR_CHAT:
                async for chunk in self._handle_mentor_chat(message):
                    yield chunk

            elif classification.action == ActionType.PROFILING_GENERAL_CHAT:
                # ìˆœì°¨ì  ì²˜ë¦¬: General Chat ì‘ë‹µ â†’ ì—°ê²°ì–´ â†’ í”„ë¡œíŒŒì¼ë§
                async for chunk in self._handle_profiling_general_chat_sequential(message):
                    yield chunk

            else:  # GENERAL_CHAT
                async for chunk in self._handle_general_chat(message):
                    yield chunk
                
        except Exception as e:
            print(f"âŒ í†µí•© ëŒ€í™” ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            yield {
                "type": "error",
                "content": f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }
    
    def clear_conversation(self):
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.conversation_history = []
        self.current_session_id = None
        print("ğŸ’¬ ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def _extract_content(self, content) -> Optional[str]:
        """ë©”ì‹œì§€ ë‚´ìš© ì¶”ì¶œ í—¬í¼ í•¨ìˆ˜"""
        if isinstance(content, str):
            return content
        elif isinstance(content, list):
            for item in content:
                if isinstance(item, dict) and "text" in item:
                    return item["text"]
        return None


# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
MultiAgentSystem = MultiMCPAgent
