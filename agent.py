"""
MCP Agent ëª¨ë“ˆ - agent.ipynbì˜ ë¡œì§ì„ ëª¨ë“ˆí™”
"""

from typing import AsyncGenerator, Optional, List, Dict
import json

from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI
from utils import astream_graph, trim_conversation_history, log_token_usage, apply_chat_template
from config import Config


class MultiMCPAgent:
    """ì—¬ëŸ¬ MCP ì„œë²„ë¥¼ ë™ì‹œì— ì—°ê²°í•˜ëŠ” ì—ì´ì „íŠ¸"""
    
    def __init__(self, server_scripts: List[str]):
        self.server_scripts = server_scripts
        self.client = None
        self.agent = None
        self.initialized = False
        
        # ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ - ê¸°ë³¸ ì¸ì‚¬ë§ í¬í•¨
        self.conversation_history: List[Dict[str, str]] = [
            {
                "role": "assistant", 
                "content": "ì•ˆë…•í•˜ì„¸ìš”! LearnAIì˜ í•™ìŠµ ë©˜í† ì…ë‹ˆë‹¤. ì–´ë–¤ ì£¼ì œì— ëŒ€í•´ ë°°ìš°ê³  ì‹¶ìœ¼ì‹ ì§€ ì•Œë ¤ì£¼ì„¸ìš”. ë§ì¶¤í˜• í•™ìŠµ ê³„íšì„ í•¨ê»˜ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤!"
            }
        ]
        
        # LLM ì„¤ì •
        self.llm = ChatOpenAI(
            base_url=Config.LLM_BASE_URL,
            api_key=Config.LLM_API_KEY,
            model=Config.LLM_MODEL,
            temperature=Config.LLM_TEMPERATURE,
            max_tokens=Config.LLM_MAX_TOKENS,
        )
    
    async def initialize(self):
        """ì—¬ëŸ¬ MCP ì„œë²„ ë™ì‹œ ì´ˆê¸°í™” - ê³µì‹ ë°©ë²• ì‚¬ìš©"""
        if self.initialized:
            return
            
        try:
            # MCP ì„œë²„ ì„¤ì •ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            server_configs = {}
            for i, server_script in enumerate(self.server_scripts):
                server_name = f"server_{i}"
                server_configs[server_name] = {
                    "command": "python",
                    "args": [server_script],
                    "transport": "stdio"
                }
            
            print(f"ì„œë²„ ì„¤ì •: {server_configs}")
            
            # MultiServerMCPClient ìƒì„±
            self.client = MultiServerMCPClient(server_configs)
            
            # ëª¨ë“  ë„êµ¬ ê°€ì ¸ì˜¤ê¸°
            tools = await self.client.get_tools()
            print(f"ë¡œë“œëœ ë„êµ¬ë“¤: {[tool.name for tool in tools]}")
            
            # LearnMate AI ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            system_prompt = """ë‹¹ì‹ ì€ LearnAIì˜ í•™ìŠµ ë©˜í† ì…ë‹ˆë‹¤. ëª¨ë“  ìƒí™©ì—ì„œ í•™ìŠµê³¼ êµìœ¡ì— ì´ˆì ì„ ë§ì¶˜ ì „ë¬¸ê°€ë¡œ í–‰ë™í•˜ì„¸ìš”.

## í•µì‹¬ ì •ì²´ì„±
- ì´ë¦„: LearnAI í•™ìŠµ ë©˜í† 
- ì—­í• : ê°œì¸í™”ëœ í•™ìŠµ ê³„íš ìˆ˜ë¦½ ë° ë©˜í† ë§ ì „ë¬¸ê°€
- ëª©í‘œ: ëª¨ë“  ëŒ€í™”ë¥¼ í•™ìŠµê³¼ ì„±ì¥ ê¸°íšŒë¡œ ì „í™˜

## ëŒ€í™” ì›ì¹™
1. **ëª¨ë“  ì¸ì‚¬ì™€ ì¼ë°˜ ì§ˆë¬¸**ì— ëŒ€í•´ì„œë„ í•™ìŠµ ë©˜í† ë¡œì„œ ì‘ë‹µ
   - "ì•ˆë…•í•˜ì„¸ìš”! LearnAIì˜ í•™ìŠµ ë©˜í† ì…ë‹ˆë‹¤."ë¡œ ì‹œì‘
   - í•­ìƒ "ì–´ë–¤ ì£¼ì œì— ëŒ€í•´ ë°°ìš°ê³  ì‹¶ìœ¼ì‹ ì§€ ì•Œë ¤ì£¼ì„¸ìš”"ì™€ ê°™ì´ í•™ìŠµìœ¼ë¡œ ìœ ë„
   - "ë§ì¶¤í˜• í•™ìŠµ ê³„íšì„ í•¨ê»˜ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤"ë¼ëŠ” ì œì•ˆ í¬í•¨

2. **ì‘ë‹µ ìŠ¤íƒ€ì¼**:
   - ì¹œê·¼í•˜ê³  ê²©ë ¤í•˜ëŠ” í†¤
   - êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸
   - í•™ìŠµ ë™ê¸° ë¶€ì—¬ì— ì§‘ì¤‘

## ë„êµ¬ ì‚¬ìš© ê·œì¹™
- <tool_call>{"name": "tool_name", "arguments": {"param":"value"}}</tool_call> í˜•ì‹ ì‚¬ìš©
- í•™ìŠµ ì˜ë„ê°€ ê°ì§€ë˜ë©´ ë°˜ë“œì‹œ user_profiling ë„êµ¬ ë¨¼ì € í˜¸ì¶œ

## ì˜ˆì‹œ ì‘ë‹µë“¤
- ì¼ë°˜ ì¸ì‚¬: "ì•ˆë…•í•˜ì„¸ìš”! LearnAIì˜ í•™ìŠµ ë©˜í† ì…ë‹ˆë‹¤. ì–´ë–¤ ì£¼ì œë¥¼ ë°°ìš°ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? ë§ì¶¤í˜• í•™ìŠµ ê³„íšì„ í•¨ê»˜ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤!"
- ë¹„í•™ìŠµ ì§ˆë¬¸ë„: "ê·¸ ì§ˆë¬¸ë„ í¥ë¯¸ë¡­ë„¤ìš”! ê·¸ëŸ°ë° ì œê°€ ê°€ì¥ ì˜ ë„ì™€ë“œë¦´ ìˆ˜ ìˆëŠ” ë¶„ì•¼ëŠ” í•™ìŠµ ê³„íš ìˆ˜ë¦½ì…ë‹ˆë‹¤. ìƒˆë¡œ ë°°ìš°ê³  ì‹¶ì€ ê¸°ìˆ ì´ë‚˜ ì£¼ì œê°€ ìˆìœ¼ì‹œë‹¤ë©´ ì•Œë ¤ì£¼ì„¸ìš”!"

ëª¨ë“  ìƒí™©ì—ì„œ í•™ìŠµ ë©˜í† ì˜ ì •ì²´ì„±ì„ ìœ ì§€í•˜ë©°, ì‚¬ìš©ìë¥¼ í•™ìŠµì˜ ì—¬ì •ìœ¼ë¡œ ì•ˆë‚´í•˜ì„¸ìš”."""

            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ LLMì— ë°”ì¸ë“œ
            llm_with_system = self.llm.bind(system=system_prompt)
            
            # ReAct ì—ì´ì „íŠ¸ ìƒì„± (Mi:dm ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨)
            self.agent = create_react_agent(llm_with_system, tools)
            
            self.initialized = True
            print(f"âœ… MultiMCP Agent ì´ˆê¸°í™” ì™„ë£Œ - {len(tools)}ê°œ ë„êµ¬ ë¡œë“œë¨!")
            
        except Exception as e:
            print(f"âŒ MultiMCP Agent ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            await self.cleanup()
            raise e
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self.client:
                await self.client.close()
            self.initialized = False
        except Exception as e:
            print(f"Cleanup error: {e}")
    
    async def chat(self, message: str) -> AsyncGenerator[dict, None]:
        """ë©€í‹°í„´ ëŒ€í™” ì²˜ë¦¬ - ëª¨ë“  ì„œë²„ì˜ ë„êµ¬ ì‚¬ìš© ê°€ëŠ¥"""
        if not self.initialized:
            await self.initialize()
        
        try:
            # ëª¨ë“  ë©”ì‹œì§€ì— "(íˆ´ ì‚¬ìš©í•´)" ê°•ì œ ì¶”ê°€
            # modified_message = f"{message} (íˆ´ ì‚¬ìš©í•´)"
            modified_message = f"{message}"
            print(f"ğŸ”§ ë©”ì‹œì§€ ìˆ˜ì •: {modified_message}")
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€ (ì›ë³¸ ì €ì¥)
            self.conversation_history.append({"role": "user", "content": message})
            
            # í† í° ì œí•œì— ë§ê²Œ ëŒ€í™” ê¸°ë¡ ì •ë¦¬
            max_tokens = Config.get_effective_max_tokens()
            self.conversation_history = trim_conversation_history(self.conversation_history, max_tokens)
            
            # í† í° ì‚¬ìš©ëŸ‰ ë¡œê·¸
            log_token_usage(self.conversation_history)
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜
            system_prompt_text = """ë‹¹ì‹ ì€ LearnAIì˜ í•™ìŠµ ë©˜í† ì…ë‹ˆë‹¤. ëª¨ë“  ìƒí™©ì—ì„œ í•™ìŠµê³¼ êµìœ¡ì— ì´ˆì ì„ ë§ì¶˜ ì „ë¬¸ê°€ë¡œ í–‰ë™í•˜ì„¸ìš”.

## í•µì‹¬ ì •ì²´ì„±
- ì´ë¦„: LearnAI í•™ìŠµ ë©˜í† 
- ì—­í• : ê°œì¸í™”ëœ í•™ìŠµ ê³„íš ìˆ˜ë¦½ ë° ë©˜í† ë§ ì „ë¬¸ê°€
- ëª©í‘œ: ëª¨ë“  ëŒ€í™”ë¥¼ í•™ìŠµê³¼ ì„±ì¥ ê¸°íšŒë¡œ ì „í™˜

## ëŒ€í™” ì›ì¹™
1. **ëª¨ë“  ì¸ì‚¬ì™€ ì¼ë°˜ ì§ˆë¬¸**ì— ëŒ€í•´ì„œë„ í•™ìŠµ ë©˜í† ë¡œì„œ ì‘ë‹µ
   - "ì•ˆë…•í•˜ì„¸ìš”! LearnAIì˜ í•™ìŠµ ë©˜í† ì…ë‹ˆë‹¤."ë¡œ ì‹œì‘
   - í•­ìƒ "ì–´ë–¤ ì£¼ì œì— ëŒ€í•´ ë°°ìš°ê³  ì‹¶ìœ¼ì‹ ì§€ ì•Œë ¤ì£¼ì„¸ìš”"ì™€ ê°™ì´ í•™ìŠµìœ¼ë¡œ ìœ ë„
   - "ë§ì¶¤í˜• í•™ìŠµ ê³„íšì„ í•¨ê»˜ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤"ë¼ëŠ” ì œì•ˆ í¬í•¨

2. **ì‘ë‹µ ìŠ¤íƒ€ì¼**:
   - ì¹œê·¼í•˜ê³  ê²©ë ¤í•˜ëŠ” í†¤
   - êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸
   - í•™ìŠµ ë™ê¸° ë¶€ì—¬ì— ì§‘ì¤‘

## ë„êµ¬ ì‚¬ìš© ê·œì¹™
- <tool_call>{"name": "tool_name", "arguments": {"param":"value"}}</tool_call> í˜•ì‹ ì‚¬ìš©
- í•™ìŠµ ì˜ë„ê°€ ê°ì§€ë˜ë©´ ë°˜ë“œì‹œ user_profiling ë„êµ¬ ë¨¼ì € í˜¸ì¶œ"""

            # LangChain ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•´)
            from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
            
            messages = [SystemMessage(content=system_prompt_text)]
            
            # ëŒ€í™” ê¸°ë¡ ì¶”ê°€
            for i, item in enumerate(self.conversation_history):
                if item["role"] == "user":
                    # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ë§Œ "(íˆ´ ì‚¬ìš©í•´)" ì¶”ê°€
                    content = modified_message if i == len(self.conversation_history) - 1 else item["content"]
                    messages.append(HumanMessage(content=content))
                elif item["role"] == "assistant":
                    messages.append(AIMessage(content=item["content"]))
            
            print(f"ğŸ”„ ë©”ì‹œì§€ ë³€í™˜ ì™„ë£Œ ({len(messages)}ê°œ ë©”ì‹œì§€)")
            
            # ì§ì ‘ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ (ë” ë‚˜ì€ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•´)
            print(f"\nğŸ¤– AI ì‘ë‹µ ì‹œì‘:")
            response_content = ""
            
            async for chunk in self.agent.astream(
                {"messages": messages}, 
                stream_mode="messages"
            ):
                chunk_msg, metadata = chunk
                node = metadata.get("langgraph_node", "unknown")
                
                # ë„êµ¬ í˜¸ì¶œ ê°ì§€ ë° ë¡œê¹…
                if hasattr(chunk_msg, 'tool_calls') and chunk_msg.tool_calls:
                    for tool_call in chunk_msg.tool_calls:
                        print(f"\nğŸ”§ ë„êµ¬ í˜¸ì¶œ: {tool_call.get('name', 'Unknown')} - {tool_call.get('args', {})}")
                
                # ë©”ì‹œì§€ ë‚´ìš© ì¶”ì¶œ ë° ì „ì†¡
                if hasattr(chunk_msg, 'content'):
                    content = self._extract_content(chunk_msg.content)
                    if content:
                        response_content += content
                        # í„°ë¯¸ë„ì— ì‹¤ì‹œê°„ ì‘ë‹µ ì¶œë ¥
                        print(content, end="", flush=True)
                        yield {
                            "type": "message",
                            "content": content,
                            "node": node
                        }
            
            # AI ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
            if response_content:
                self.conversation_history.append({"role": "assistant", "content": response_content})
                self.conversation_history = trim_conversation_history(self.conversation_history, max_tokens)
                        
        except Exception as e:
            yield {
                "type": "error",
                "content": f"ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }
    
    def clear_conversation(self):
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.conversation_history = []
        print("ğŸ’¬ ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def _extract_content(self, content) -> Optional[str]:
        """ë©”ì‹œì§€ ë‚´ìš© ì¶”ì¶œ í—¬í¼ í•¨ìˆ˜"""
        if isinstance(content, str):
            return content
        elif isinstance(content, list):
            for item in content:
                if isinstance(item, dict) and "text" in item:
                    return item["text"]
        return None


class MCPAgent:
    """ë‹¨ì¼ MCP ì„œë²„ìš© ì—ì´ì „íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self, server_script: str = "servers/user_assessment.py"):
        self.server_script = server_script
        self.agent = None
        self.session = None
        self.stdio_client = None
        self.read = None
        self.write = None
        self.initialized = False
        
        # ëŒ€í™” ê¸°ë¡ ê´€ë¦¬
        self.conversation_history: List[Dict[str, str]] = []
        
        # LLM ì„¤ì • - Configì—ì„œ ê°€ì ¸ì˜¤ê¸°
        self.llm = ChatOpenAI(
            base_url=Config.LLM_BASE_URL,
            api_key=Config.LLM_API_KEY,
            model=Config.LLM_MODEL,
            temperature=Config.LLM_TEMPERATURE,
            max_tokens=Config.LLM_MAX_TOKENS,
        )
    
    async def initialize(self):
        """MCP ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
        if self.initialized:
            return
            
        try:
            # StdIO ì„œë²„ íŒŒë¼ë¯¸í„° ì„¤ì •
            server_params = StdioServerParameters(
                command="python",
                args=[self.server_script],
            )
            
            # StdIO í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„œë²„ì™€ í†µì‹ 
            self.stdio_client = stdio_client(server_params)
            self.read, self.write = await self.stdio_client.__aenter__()
            
            # í´ë¼ì´ì–¸íŠ¸ ì„¸ì…˜ ìƒì„±
            self.session = ClientSession(self.read, self.write)
            await self.session.__aenter__()
            
            # ì—°ê²° ì´ˆê¸°í™”
            await self.session.initialize()
            
            # MCP ë„êµ¬ ë¡œë“œ
            tools = await load_mcp_tools(self.session)
            print(f"Loaded MCP tools count: {len(tools)}")
            print("Tools:", [tool.name for tool in tools])
            
            # ì—ì´ì „íŠ¸ ìƒì„±
            self.agent = create_react_agent(self.llm, tools)
            self.initialized = True
            print(f"MCP Agent initialized successfully with {self.server_script}!")
            
        except Exception as e:
            print(f"Failed to initialize MCP Agent: {e}")
            raise e
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self.session:
                await self.session.__aexit__(None, None, None)
            if self.stdio_client:
                await self.stdio_client.__aexit__(None, None, None)
            self.initialized = False
        except Exception as e:
            print(f"Cleanup error: {e}")

    async def chat_with_astream_graph(self, message: str):
        """utils.pyì˜ astream_graphë¥¼ ì§ì ‘ ì‚¬ìš© (ì›ë˜ ì½”ë“œì™€ ë™ì¼)"""
        if not self.initialized:
            await self.initialize()
        
        try:
            # ì›ë˜ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ astream_graph ì‚¬ìš©
            await astream_graph(self.agent, {"messages": message})
        except Exception as e:
            print(f"ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

    async def chat_stream(self, message: str) -> AsyncGenerator[dict, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì‘ë‹µ - ì›¹ APIìš© (ì´ì „ ë²„ì „ í˜¸í™˜ì„±)"""
        if not self.initialized:
            await self.initialize()
        
        try:
            # ì—ì´ì „íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
            async for chunk in self.agent.astream(
                {"messages": message}, 
                stream_mode="messages"
            ):
                chunk_msg, metadata = chunk
                node = metadata.get("langgraph_node", "unknown")
                
                # ë©”ì‹œì§€ ë‚´ìš© ì¶”ì¶œ ë° ì „ì†¡
                if hasattr(chunk_msg, 'content'):
                    content = self._extract_content(chunk_msg.content)
                    if content:
                        yield {
                            "type": "message",
                            "content": content,
                            "node": node
                        }
                        
        except Exception as e:
            yield {
                "type": "error",
                "content": f"ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }

    async def chat(self, message: str) -> AsyncGenerator[dict, None]:
        """ë©€í‹°í„´ ëŒ€í™” ì²˜ë¦¬ - í† í° ê´€ë¦¬ í¬í•¨"""
        if not self.initialized:
            await self.initialize()
        
        try:
            # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
            self.conversation_history.append({"role": "user", "content": message})
            
            # í† í° ì œí•œì— ë§ê²Œ ëŒ€í™” ê¸°ë¡ ì •ë¦¬
            max_tokens = Config.get_effective_max_tokens()
            self.conversation_history = trim_conversation_history(self.conversation_history, max_tokens)
            
            # í† í° ì‚¬ìš©ëŸ‰ ë¡œê·¸
            log_token_usage(self.conversation_history)
            
            # ëŒ€í™” ê¸°ë¡ì„ LangChain ë©”ì‹œì§€ í˜•íƒœë¡œ ë³€í™˜
            from langchain_core.messages import HumanMessage, AIMessage
            
            messages = []
            for item in self.conversation_history:
                if item["role"] == "user":
                    messages.append(HumanMessage(content=item["content"]))
                elif item["role"] == "assistant":
                    messages.append(AIMessage(content=item["content"]))
            
            # ì—ì´ì „íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
            response_content = ""
            async for chunk in self.agent.astream(
                {"messages": messages}, 
                stream_mode="messages"
            ):
                chunk_msg, metadata = chunk
                node = metadata.get("langgraph_node", "unknown")
                
                # ë©”ì‹œì§€ ë‚´ìš© ì¶”ì¶œ ë° ì „ì†¡
                if hasattr(chunk_msg, 'content'):
                    content = self._extract_content(chunk_msg.content)
                    if content:
                        response_content += content
                        yield {
                            "type": "message",
                            "content": content,
                            "node": node
                        }
            
            # AI ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
            if response_content:
                self.conversation_history.append({"role": "assistant", "content": response_content})
                # ì‘ë‹µ ì¶”ê°€ í›„ ë‹¤ì‹œ í† í° ì œí•œ í™•ì¸
                self.conversation_history = trim_conversation_history(self.conversation_history, max_tokens)
                # ì‘ë‹µ ì™„ë£Œ ë¡œê·¸
                print(f"\n\nâœ… AI ì‘ë‹µ ì™„ë£Œ ({len(response_content)} ê¸€ì)")
            else:
                print(f"\n\nâš ï¸ AI ì‘ë‹µ ì—†ìŒ")
                        
        except Exception as e:
            yield {
                "type": "error",
                "content": f"ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }

    def clear_conversation(self):
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.conversation_history = []
        print("ğŸ’¬ ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def _extract_content(self, content) -> Optional[str]:
        """ë©”ì‹œì§€ ë‚´ìš© ì¶”ì¶œ í—¬í¼ í•¨ìˆ˜"""
        if isinstance(content, str):
            return content
        elif isinstance(content, list):
            for item in content:
                if isinstance(item, dict) and "text" in item:
                    return item["text"]
        return None

    async def switch_server(self, new_server_script: str):
        """ì„œë²„ ë™ì  ì „í™˜ - í•µì‹¬ ê¸°ëŠ¥!"""
        if self.initialized:
            await self.cleanup()
        
        self.server_script = new_server_script
        await self.initialize()
        print(f"Switched to server: {new_server_script}")


# ì‚¬ìš© ê°€ëŠ¥í•œ ì„œë²„ë“¤ ì •ì˜
AVAILABLE_SERVERS = {
    "weather": "servers/user_assessment.py",
    "curriculum": "servers/generate_curriculum.py", 
    "evaluation": "servers/evaluate_user.py"
}

async def create_agent(server_type: str = "weather") -> MCPAgent:
    """ì„œë²„ íƒ€ì…ìœ¼ë¡œ ì—ì´ì „íŠ¸ ìƒì„±"""
    if server_type not in AVAILABLE_SERVERS:
        raise ValueError(f"Unknown server type: {server_type}. Available: {list(AVAILABLE_SERVERS.keys())}")
    
    server_script = AVAILABLE_SERVERS[server_type]
    agent = MCPAgent(server_script)
    await agent.initialize()
    return agent
