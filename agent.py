"""
MCP Agent ëª¨ë“ˆ - Stateful Multi-Agent Systemê³¼ ì—°ë™
"""

from typing import AsyncGenerator, Optional, List, Dict
import json
import re
from pydantic import BaseModel, Field
from enum import Enum

from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI
from utils import astream_graph, trim_conversation_history, log_token_usage, apply_chat_template
from config import Config

class ActionType(str, Enum):
    """ì‚¬ìš©ì ë©”ì‹œì§€ì— ëŒ€í•œ ì•¡ì…˜ ìœ í˜•"""
    GENERAL_CHAT = "general_chat"           # ì¼ë°˜ ëŒ€í™”
    USER_PROFILING = "user_profiling"       # í•™ìŠµ í”„ë¡œí•„ ìˆ˜ì§‘ í•„ìš”
    GENERATE_CURRICULUM = "generate_curriculum"  # ì»¤ë¦¬í˜ëŸ¼ ìƒì„±
    PROFILING_GENERAL_CHAT = "profiling_general_chat"  # í”„ë¡œíŒŒì¼ë§ ì¤‘ ì¼ë°˜ ëŒ€í™”

class ActionClassification(BaseModel):
    """ì•¡ì…˜ ë¶„ë¥˜ ê²°ê³¼"""
    action: ActionType = Field(description="ìˆ˜í–‰í•  ì•¡ì…˜ íƒ€ì…")
    
class MultiMCPAgent:
    """ì—¬ëŸ¬ MCP ì„œë²„ë¥¼ ë™ì‹œì— ì—°ê²°í•˜ëŠ” ì—ì´ì „íŠ¸ with Stateful Assessment"""
    
    def __init__(self, server_scripts: List[str]):
        self.server_scripts = server_scripts
        self.client = None
        self.agent = None
        self.initialized = False
        
        # ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬
        self.current_session_id = None
        
        # ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ - ê¸°ë³¸ ì¸ì‚¬ë§ í¬í•¨
        self.conversation_history: List[Dict[str, str]] = [
            {
                "role": "assistant", 
                "content": "ì•ˆë…•í•˜ì„¸ìš”! LearnAI ì…ë‹ˆë‹¤. ì–´ë–¤ ì£¼ì œì— ëŒ€í•´ ë°°ìš°ê³  ì‹¶ìœ¼ì‹ ì§€ ì•Œë ¤ì£¼ì‹œë©´ ë§ì¶¤í˜• í•™ìŠµ ê³„íšì„ í•¨ê»˜ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤!"
            }
        ]
        self.max_tokens = Config.LLM_MAX_TOKENS

        # LLM ì„¤ì •
        self.llm = ChatOpenAI(
            base_url=Config.LLM_BASE_URL,
            api_key=Config.LLM_API_KEY,
            model=Config.LLM_MODEL,
            temperature=Config.LLM_TEMPERATURE,
            max_tokens=self.max_tokens,
            model_kwargs={"max_completion_tokens": None}  # Friendli.aiì—ì„œ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„° ì œê±°
        )
         
    async def initialize(self):
        """ì—¬ëŸ¬ MCP ì„œë²„ ë™ì‹œ ì´ˆê¸°í™” - ê³µì‹ ë°©ë²• ì‚¬ìš©"""
        if self.initialized:
            return
            
        try:
            # MCP ì„œë²„ ì„¤ì •ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            server_configs = {}
            for i, server_script in enumerate(self.server_scripts):
                server_name = f"server_{i}"
                server_configs[server_name] = {
                    "command": "python",
                    "args": [server_script],
                    "transport": "stdio"
                }
            
            print(f"ì„œë²„ ì„¤ì •: {server_configs}")
            
            # MultiServerMCPClient ìƒì„±
            self.client = MultiServerMCPClient(server_configs)
            
            # ëª¨ë“  ë„êµ¬ ê°€ì ¸ì˜¤ê¸°
            tools = await self.client.get_tools()
            print(f"ë¡œë“œëœ ë„êµ¬ë“¤: {[tool.name for tool in tools]}")
            
            # ReAct ì—ì´ì „íŠ¸ ìƒì„±
            self.agent = create_react_agent(self.llm, tools)
            
            self.initialized = True
            print(f"âœ… MultiMCP Agent ì´ˆê¸°í™” ì™„ë£Œ - {len(tools)}ê°œ ë„êµ¬ ë¡œë“œë¨!")
            
        except Exception as e:
            print(f"âŒ MultiMCP Agent ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            await self.cleanup()
            raise e
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self.client:
                # MultiServerMCPClientëŠ” close ë©”ì„œë“œê°€ ì—†ìœ¼ë¯€ë¡œ ì œê±°
                pass
            self.initialized = False
        except Exception as e:
            print(f"Cleanup error: {e}")
    
    def _extract_session_id(self, response_content: str) -> Optional[str]:
        """ì‘ë‹µì—ì„œ ì„¸ì…˜ ID ì¶”ì¶œ"""
        session_match = re.search(r'Session:\s*([a-zA-Z0-9-]+)', response_content)
        if session_match:
            return session_match.group(1)
        return None
    
    
    async def chat(self, message: str) -> AsyncGenerator[dict, None]:
        """ë©€í‹°í„´ ëŒ€í™” ì²˜ë¦¬ - Stateful Assessment ì§€ì›"""
        if not self.initialized:
            await self.initialize()
        
        try:
            print(f"ğŸ“ ì‚¬ìš©ì ë©”ì‹œì§€: {message}")
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€ (ì›ë³¸ ì €ì¥)
            self.conversation_history.append({"role": "user", "content": message})
            
            # í† í° ì œí•œì— ë§ê²Œ ëŒ€í™” ê¸°ë¡ ì •ë¦¬
            self.conversation_history = trim_conversation_history(self.conversation_history, self.max_tokens)
            
            # í† í° ì‚¬ìš©ëŸ‰ ë¡œê·¸
            log_token_usage(self.conversation_history)
            
            # MCP ë°©ì‹: ReAct ì—ì´ì „íŠ¸ê°€ í•„ìš”í•œ ë„êµ¬ë¥¼ ìë™ìœ¼ë¡œ ì„ íƒ
            async for chunk in self._handle_unified_conversation(message):
                yield chunk
                        
        except Exception as e:
            yield {
                "type": "error",
                "content": f"ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }
    
    
    async def _classify_user_intent(self, message: str) -> ActionClassification:
        """ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë¶„ë¥˜í•˜ì—¬ ì ì ˆí•œ ì•¡ì…˜ ê²°ì •"""

        # í”„ë¡œíŒŒì¼ë§ ìƒíƒœ í™•ì¸
        profiling_status = await self._get_profiling_status()

        # í”„ë¡œíŒŒì¼ë§ ì§„í–‰ ì¤‘ì¸ ê²½ìš°
        if profiling_status["in_progress"]:
            print(f"ğŸ“Š í”„ë¡œíŒŒì¼ë§ ì§„í–‰ ì¤‘ (ì™„ë£Œìœ¨: {profiling_status['completion_rate']*100:.0f}%)")

            # ìµœê·¼ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ í¬í•¨ (AI ì§ˆë¬¸ + ì‚¬ìš©ì ë‹µë³€ ë§¥ë½ íŒŒì•…)
            recent_context = ""
            if len(self.conversation_history) >= 2:
                # ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ì™€ í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€
                last_ai = self.conversation_history[-1] if self.conversation_history[-1]["role"] == "assistant" else None
                if last_ai:
                    recent_context = f"AI ì§ˆë¬¸: {last_ai['content'][:100]}...\n"

            classification_prompt = f"""í”„ë¡œíŒŒì¼ë§ ì¤‘ì¸ ì‚¬ìš©ìì˜ ë©”ì‹œì§€ë¥¼ ë¶„ë¥˜í•˜ì„¸ìš”.

{recent_context}ì‚¬ìš©ì ë‹µë³€: "{message}"

í˜„ì¬ ìˆ˜ì§‘ëœ ì •ë³´:
- í•™ìŠµ ì£¼ì œ: {profiling_status.get('topic', 'ë¯¸ìˆ˜ì§‘')}
- ìˆ˜ì¤€/ì‹œê°„: {profiling_status.get('constraints', 'ë¯¸ìˆ˜ì§‘')}
- í•™ìŠµ ëª©í‘œ: {profiling_status.get('goal', 'ë¯¸ìˆ˜ì§‘')}

ë¶„ë¥˜ ê¸°ì¤€:

1. **profiling_general_chat** (ìš°ì„  ì²´í¬): ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì— í•´ë‹¹í•˜ë©´ ë¬´ì¡°ê±´ ì´ê²ƒìœ¼ë¡œ ë¶„ë¥˜
   - ìˆœìˆ˜ ì¸ì‚¬: "ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”", "í•˜ì´", "hi", "hello"
   - ê°ì‚¬ í‘œí˜„: "ê³ ë§ˆì›Œ", "ê°ì‚¬í•´", "thanks", "ê³ ë§™ìŠµë‹ˆë‹¤"
   - ì‘ë³„ ì¸ì‚¬: "ì˜ê°€", "ë°”ì´", "bye", "ì•ˆë…•íˆ"
   - ì™„ì „ ì¼ìƒ: "ë‚ ì”¨ ì–´ë•Œ?", "ë­í•´?", "ì˜ì§€ë‚´?"

2. **user_profiling**: ìœ„ì— í•´ë‹¹í•˜ì§€ ì•Šê³  í•™ìŠµ ê´€ë ¨ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°
   - í•™ìŠµ ì£¼ì œ: íŒŒì´ì¬, ìë°”, ì˜ì–´, ì™¸êµ­ì–´, ë°ì´í„°ë¶„ì„ ë“±
   - ìˆ˜ì¤€/ê²½í—˜: ì´ˆë³´, 2ë…„ ê²½í—˜, ê¸°ì´ˆëŠ” ì•Œì•„ ë“±
   - í•™ìŠµ ëª©í‘œ/ì´ìœ : ì·¨ì—…, ì´ì§, í”„ë¡œì íŠ¸, ì¹œêµ¬ë“¤ê³¼ ëŒ€í™”, ì—…ë¬´ì— í•„ìš”í•´ì„œ ë“±
   - í•™ìŠµ ì‹œê°„: ì£¼ 3ì‹œê°„, ë§¤ì¼ 1ì‹œê°„ ë“±

**ì¤‘ìš”**: ë¨¼ì € profiling_general_chatì„ ì²´í¬í•˜ê³ , í•´ë‹¹í•˜ì§€ ì•Šìœ¼ë©´ user_profilingìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”."""

            try:
                from pydantic import BaseModel, Field
                from enum import Enum

                class ProfilingAction(str, Enum):
                    USER_PROFILING = "user_profiling"  # í•™ìŠµ ì •ë³´ ì œê³µ
                    PROFILING_GENERAL_CHAT = "profiling_general_chat"  # ì¼ë°˜ ëŒ€í™”

                class ProfilingClassification(BaseModel):
                    action: ProfilingAction = Field(
                        description="user_profiling(í•™ìŠµì£¼ì œ/ìˆ˜ì¤€/ëª©í‘œ ê´€ë ¨) ë˜ëŠ” profiling_general_chat(ì¼ìƒëŒ€í™”)"
                    )

                classifier = self.llm.with_structured_output(ProfilingClassification)
                result = classifier.invoke(classification_prompt)

                print(f"ğŸ” ë¶„ë¥˜: {result.action}")

                if result.action == ProfilingAction.USER_PROFILING:
                    return ActionClassification(action=ActionType.USER_PROFILING)
                else:
                    return ActionClassification(action=ActionType.PROFILING_GENERAL_CHAT)

            except Exception as e:
                print(f"âŒ ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
                return ActionClassification(action=ActionType.USER_PROFILING)

        # í”„ë¡œíŒŒì¼ë§ ì™„ë£Œëœ ê²½ìš°
        else:
            print(f"âœ… í”„ë¡œíŒŒì¼ë§ ì™„ë£Œ ìƒíƒœ")

            classification_prompt = f"""ì‚¬ìš©ì ë©”ì‹œì§€ì˜ ì˜ë„ë¥¼ ë¶„ë¥˜í•˜ì„¸ìš”.

ì‚¬ìš©ì ë©”ì‹œì§€: "{message}"

ë¶„ë¥˜ ê¸°ì¤€:
1. **generate_curriculum**: ì»¤ë¦¬í˜ëŸ¼/í•™ìŠµê³„íš ìƒì„± ìš”ì²­ ë˜ëŠ” ê¸ì •ì  ì‘ë‹µ
   - ì˜ˆ: "ì»¤ë¦¬í˜ëŸ¼ ë§Œë“¤ì–´ì¤˜", "í•™ìŠµ ê³„íš ì„¸ì›Œì¤˜", "ë¡œë“œë§µ ë³´ì—¬ì¤˜"
   - ì˜ˆ: "ì‘", "ì¢‹ì•„", "ì‹œì‘í•´ì¤˜", "ë„¤", "ê·¸ë˜", "í•´ì¤˜", "ë§Œë“¤ì–´ì¤˜"
   - ì˜ˆ: "ë§ì¶¤í˜• ê³„íš ë§Œë“¤ì–´ì¤˜", "ìƒì„±í•´ì¤˜", "ì‹œì‘í•˜ì"

2. **user_profiling**: ìƒˆë¡œìš´ í•™ìŠµ ì£¼ì œ ë˜ëŠ” í”„ë¡œí•„ ìˆ˜ì •
   - ì˜ˆ: "ë‹¤ë¥¸ ê²ƒë„ ë°°ìš°ê³  ì‹¶ì–´", "ëª©í‘œê°€ ë°”ë€Œì—ˆì–´", "ì•„ë‹ˆ ë‹¤ì‹œ í• ê²Œ"

3. **general_chat**: ì¼ë°˜ ëŒ€í™” (ì»¤ë¦¬í˜ëŸ¼ê³¼ ë¬´ê´€í•œ)
   - ì˜ˆ: "ê³ ë§ˆì›Œ", "ì•ˆë…•", "ë­í•˜ê³  ìˆì–´?"

**ì¤‘ìš”**: í”„ë¡œíŒŒì¼ë§ ì™„ë£Œ í›„ ê¸ì •ì ì¸ ì‘ë‹µì€ ëŒ€ë¶€ë¶„ generate_curriculumìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”."""

            try:
                classifier = self.llm.with_structured_output(ActionClassification)
                result = classifier.invoke(classification_prompt)
                print(f"ğŸ” ì˜ë„ ë¶„ë¥˜: {result.action}")
                return result
            except Exception as e:
                print(f"âŒ ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
                return ActionClassification(action=ActionType.GENERAL_CHAT)

    async def _handle_user_profiling(self, message: str) -> AsyncGenerator[dict, None]:
        """user_profiling ë„êµ¬ë¥¼ ì‚¬ìš©í•œ í”„ë¡œí•„ ìˆ˜ì§‘"""
        print(f"ğŸ“Š ì‚¬ìš©ì í”„ë¡œí•„ë§ ì‹œì‘")
        
        try:
            # user_profiling ë„êµ¬ ì°¾ê¸°
            tools = await self.client.get_tools()
            user_profiling_tool = next((tool for tool in tools if tool.name == "user_profiling"), None)
            
            if not user_profiling_tool:
                yield {"type": "error", "content": "ì‚¬ìš©ì í”„ë¡œí•„ë§ ë„êµ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
                return
            
            # ë„êµ¬ ì‹¤í–‰
            tool_args = {"user_message": message, "session_id": self.current_session_id}
            print(f"ğŸ”§ user_profiling í˜¸ì¶œ: {tool_args}")
            
            result = await user_profiling_tool.ainvoke(tool_args)

            if result:
                # ê¸€ìë³„ë¡œ ìŠ¤íŠ¸ë¦¬ë°ì²˜ëŸ¼ ì¶œë ¥ (ê°ê° ê°œë³„ ì „ì†¡)
                import asyncio

                for char in result:
                    print(char, end="", flush=True)
                    yield {"type": "message", "content": char, "node": "user_profiling"}
                    await asyncio.sleep(0.05)  # ê¸€ìë³„ ë”œë ˆì´

                # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ì‹ í˜¸
                yield {"type": "streaming_complete", "node": "user_profiling"}

                self.conversation_history.append({"role": "assistant", "content": result})

                # ë„êµ¬ í˜¸ì¶œ í›„ ìµœì‹  í”„ë¡œí•„ ì •ë³´ ë¡œë“œ
                profile_data = None
                try:
                    from servers.user_assessment import load_session
                    if self.current_session_id:
                        session_data = load_session(self.current_session_id)
                        if session_data:
                            profile_info = {
                                'topic': session_data.get('topic', ''),
                                'constraints': session_data.get('constraints', ''),
                                'goal': session_data.get('goal', '')
                            }
                            profile_data = {k: v for k, v in profile_info.items() if v}
                            if profile_data:  # ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œë§Œ ì¶œë ¥
                                print(f"ğŸ“Š í˜„ì¬ í”„ë¡œí•„: {profile_data}")
                except Exception as e:
                    print(f"í”„ë¡œí•„ ë¡œë“œ ì˜¤ë¥˜: {e}")

                # ìµœì¢… ì™„ì„±ëœ ì‘ë‹µ ì „ì†¡
                response_data = {"type": "message", "content": result, "node": "user_profiling"}
                if profile_data:
                    response_data["profile"] = profile_data

                yield response_data
                
        except Exception as e:
            print(f"âŒ ì‚¬ìš©ì í”„ë¡œí•„ë§ ì˜¤ë¥˜: {e}")
            yield {"type": "error", "content": f"í”„ë¡œí•„ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}

    async def _handle_curriculum_generation(self, message: str) -> AsyncGenerator[dict, None]:
        """generate_curriculum_from_session ë„êµ¬ë¥¼ ì‚¬ìš©í•œ ì»¤ë¦¬í˜ëŸ¼ ìƒì„±"""
        print(f"ğŸ“š ì»¤ë¦¬í˜ëŸ¼ ìƒì„± ì‹œì‘")
        
        try:
            # generate_curriculum_from_session ë„êµ¬ ì°¾ê¸°
            tools = await self.client.get_tools()
            curriculum_tool = next((tool for tool in tools if tool.name == "generate_curriculum_from_session"), None)
            
            if not curriculum_tool:
                yield {"type": "error", "content": "ì»¤ë¦¬í˜ëŸ¼ ìƒì„± ë„êµ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
                return
            
            # ë„êµ¬ ì‹¤í–‰ (ì‚¬ìš©ì ë©”ì‹œì§€ë„ ì „ë‹¬)
            tool_args = {
                "session_id": self.current_session_id,
                "user_message": message
            }
            print(f"ğŸ”§ generate_curriculum_from_session í˜¸ì¶œ: {tool_args}")
            
            result = await curriculum_tool.ainvoke(tool_args)

            if result:
                print(result, end="", flush=True)
                self.conversation_history.append({"role": "assistant", "content": result})

                # íƒ­ ì „í™˜ ì‹ í˜¸ ë¨¼ì € ì „ì†¡
                yield {"type": "curriculum_created", "content": "âœ… ë§ì¶¤í˜• ì»¤ë¦¬í˜ëŸ¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤! ìƒë‹¨ì˜ \"ë‚˜ì˜ ì»¤ë¦¬í˜ëŸ¼\" íƒ­ì—ì„œ í™•ì¸í•´ë³´ì„¸ìš”."}

                # ì¼ë°˜ ë©”ì‹œì§€ë„ ì „ì†¡
                yield {"type": "message", "content": result, "node": "generate_curriculum"}
                
        except Exception as e:
            print(f"âŒ ì»¤ë¦¬í˜ëŸ¼ ìƒì„± ì˜¤ë¥˜: {e}")
            yield {"type": "error", "content": f"ì»¤ë¦¬í˜ëŸ¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}

    async def _handle_general_chat(self, message: str) -> AsyncGenerator[dict, None]:
        """ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬ (ë„êµ¬ ì—†ì´)"""
        print(f"ğŸ’¬ ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬")
        
        try:
            # LearnAI ì„±ê²©ì˜ ì¼ë°˜ ëŒ€í™” í”„ë¡¬í”„íŠ¸ - í•™ìŠµìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ìœ ë„
            system_prompt = """ë‹¹ì‹ ì€ LearnMateì˜ ì¹œê·¼í•œ í•™ìŠµ ë©˜í† ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì¼ë°˜ì ì¸ ëŒ€í™”(ì¸ì‚¬, ì•ˆë¶€, ê°ì‚¬ ë“±)ì— ìì—°ìŠ¤ëŸ½ê²Œ ì‘ë‹µí•œ í›„,
ë°˜ë“œì‹œ í•™ìŠµ ê´€ë ¨ ì§ˆë¬¸ìœ¼ë¡œ ëŒ€í™”ë¥¼ ìœ ë„í•˜ì„¸ìš”.

ì‘ë‹µ êµ¬ì¡°:
1. ì‚¬ìš©ì ë©”ì‹œì§€ì— ëŒ€í•œ ì ì ˆí•œ ì¼ë°˜ ì‘ë‹µ (1-2ë¬¸ì¥)
2. ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°ì–´ ì‚¬ìš©
3. í•™ìŠµ ê´€ë ¨ ì§ˆë¬¸ìœ¼ë¡œ ìœ ë„ (ì˜ˆ: "í˜¹ì‹œ ìš”ì¦˜ ë°°ìš°ê³  ì‹¶ì€ ê²ƒì´ ìˆìœ¼ì‹ ê°€ìš”?", "ìƒˆë¡œ ë„ì „í•´ë³´ê³  ì‹¶ì€ ë¶„ì•¼ëŠ” ì—†ìœ¼ì‹ ê°€ìš”?")

ì˜ˆì‹œ:
- ì‚¬ìš©ì: "ì•ˆë…•í•˜ì„¸ìš”" â†’ "ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤. í˜¹ì‹œ ì˜¤ëŠ˜ ìƒˆë¡œ ë°°ì›Œë³´ê³  ì‹¶ì€ ê²ƒì´ ìˆìœ¼ì‹ ê°€ìš”?"
- ì‚¬ìš©ì: "ê³ ë§ˆì›Œ" â†’ "ì²œë§Œì—ìš”! ê·¸ëŸ°ë° í˜¹ì‹œ ìš”ì¦˜ ê´€ì‹¬ ìˆëŠ” í•™ìŠµ ë¶„ì•¼ê°€ ìˆìœ¼ì‹ ê°€ìš”?"

LearnMateëŠ” í•™ìŠµ ì„œë¹„ìŠ¤ì´ë¯€ë¡œ í•­ìƒ í•™ìŠµ ë°©í–¥ìœ¼ë¡œ ëŒ€í™”ë¥¼ ì´ëŒì–´ì•¼ í•©ë‹ˆë‹¤."""

            from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
            
            messages = [SystemMessage(content=system_prompt)]
            
            # ìµœê·¼ ëŒ€í™” ê¸°ë¡ë§Œ í¬í•¨ (í† í° ì ˆì•½)
            for item in self.conversation_history[-4:]:
                if item["role"] == "user":
                    messages.append(HumanMessage(content=item["content"]))
                elif item["role"] == "assistant":
                    messages.append(AIMessage(content=item["content"]))
            
            # LLM ì§ì ‘ í˜¸ì¶œ (ë„êµ¬ ì—†ì´)
            response_content = ""
            async for chunk in self.llm.astream(messages):
                if hasattr(chunk, 'content') and chunk.content:
                    response_content += chunk.content
                    print(chunk.content, end="", flush=True)
                    yield {"type": "message", "content": chunk.content, "node": "general_chat"}
            
            if response_content:
                self.conversation_history.append({"role": "assistant", "content": response_content})
                
        except Exception as e:
            print(f"âŒ ì¼ë°˜ ëŒ€í™” ì˜¤ë¥˜: {e}")
            yield {"type": "error", "content": f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}

    async def _handle_profiling_general_chat_sequential(self, message: str) -> AsyncGenerator[dict, None]:
        """í”„ë¡œíŒŒì¼ë§ ì¤‘ general chat ìˆœì°¨ ì²˜ë¦¬: ì¼ë°˜ ì‘ë‹µ â†’ ì—°ê²° â†’ í”„ë¡œíŒŒì¼ë§"""
        print(f"ğŸ”„ í”„ë¡œíŒŒì¼ë§ ì¤‘ ì¼ë°˜ ëŒ€í™” ìˆœì°¨ ì²˜ë¦¬")

        try:
            # 1ë‹¨ê³„: í”„ë¡œíŒŒì¼ë§ì´ ì§„í–‰ ì¤‘ì¸ì§€ í™•ì¸
            profiling_status = await self._get_profiling_status()

            if not profiling_status["in_progress"]:
                # í”„ë¡œíŒŒì¼ë§ì´ ì§„í–‰ì¤‘ì´ ì•„ë‹ˆë¼ë©´ ì¼ë°˜ ëŒ€í™”ë¡œ ì²˜ë¦¬
                async for chunk in self._handle_general_chat(message):
                    yield chunk
                return

            # 2ë‹¨ê³„: í”„ë¡œíŒŒì¼ë§ ìƒíƒœ í™•ì¸ ë° ì§„í–‰ë¥  í‘œì‹œ
            topic = profiling_status.get("topic", "")
            constraints = profiling_status.get("constraints", "")
            goal = profiling_status.get("goal", "")

            # ì§„í–‰ë¥  ê³„ì‚°
            missing_info = []
            progress_items = []

            if not topic:
                missing_info.append("í•™ìŠµ ì£¼ì œ")
                progress_items.append("âŒ í•™ìŠµ ì£¼ì œ")
            else:
                progress_items.append(f"âœ… í•™ìŠµ ì£¼ì œ: {topic}")

            level_keywords = ["ì´ˆë³´", "ì¤‘ê¸‰", "ê³ ê¸‰", "ìˆ˜ì¤€", "ê²½í—˜", "ì²˜ìŒ", "ì…ë¬¸", "ê¸°ì´ˆ"]
            has_level = any(kw in constraints for kw in level_keywords)

            if not has_level:
                missing_info.append("í˜„ì¬ ìˆ˜ì¤€")
                progress_items.append("âŒ í˜„ì¬ ìˆ˜ì¤€")
            else:
                level_part = next((part for part in constraints.split(',') if any(kw in part for kw in level_keywords)), constraints)
                progress_items.append(f"âœ… í˜„ì¬ ìˆ˜ì¤€: {level_part.strip()}")

            if not goal:
                missing_info.append("í•™ìŠµ ëª©í‘œ")
                progress_items.append("âŒ í•™ìŠµ ëª©í‘œ")
            else:
                progress_items.append(f"âœ… í•™ìŠµ ëª©í‘œ: {goal}")

            completed_count = 3 - len(missing_info)
            progress_bar = "ğŸŸ©" * completed_count + "â¬œ" * len(missing_info)

            # 3ë‹¨ê³„: ìì—°ìŠ¤ëŸ¬ìš´ í†µí•© ì‘ë‹µ ìƒì„±
            next_needed = missing_info[0] if missing_info else None

            integrated_prompt = f"""ì‚¬ìš©ìê°€ ì¼ë°˜ì ì¸ ëŒ€í™”ë¥¼ í–ˆìŠµë‹ˆë‹¤: "{message}"

ì´ì— ëŒ€í•´ ì¹œê·¼í•˜ê²Œ ì‘ë‹µí•œ í›„, ìì—°ìŠ¤ëŸ½ê²Œ í•™ìŠµ ê´€ë ¨ ì§ˆë¬¸ìœ¼ë¡œ ì—°ê²°í•´ì£¼ì„¸ìš”.

í˜„ì¬ ìƒí™©:
- í•™ìŠµ ì£¼ì œ: {"íŒŒì•…ë¨ (" + topic + ")" if topic else "ì•„ì§ í•„ìš”"}
- í˜„ì¬ ìˆ˜ì¤€: {"íŒŒì•…ë¨" if has_level else "ì•„ì§ í•„ìš”"}
- í•™ìŠµ ëª©í‘œ: {"íŒŒì•…ë¨ (" + goal + ")" if goal else "ì•„ì§ í•„ìš”"}

{f"ë‹¤ìŒì— ì•Œì•„ë´ì•¼ í•  ê²ƒ: {next_needed}" if next_needed else ""}

**ì¤‘ìš”**: ì´ë¯¸ í•™ìŠµ ì£¼ì œê°€ "{topic}"ë¡œ ì •í•´ì ¸ ìˆìŠµë‹ˆë‹¤. ì ˆëŒ€ ë‹¤ë¥¸ ì£¼ì œë¥¼ ë¬»ì§€ ë§ê³ , ë°˜ë“œì‹œ {topic}ì— ëŒ€í•œ {next_needed if next_needed else "ì¶”ê°€ ì •ë³´"}ë¥¼ ë¬¼ì–´ë³´ì„¸ìš”.

**í•„ìˆ˜ ìš”êµ¬ì‚¬í•­**: ì‘ë‹µ ë§ˆì§€ë§‰ì— ë°˜ë“œì‹œ "{topic}ì— ëŒ€í•œ ì§ˆë¬¸"ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

ìš”êµ¬ì‚¬í•­:
1. ë¨¼ì € ì‚¬ìš©ìì˜ ë§ì— ê³µê°í•˜ê³  ì¹œê·¼í•˜ê²Œ ë°˜ì‘
2. ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°ì–´ë‚˜ ë¬¸ì¥ìœ¼ë¡œ í•™ìŠµ ê´€ë ¨ ì§ˆë¬¸ìœ¼ë¡œ ì´ì–´ê°€ê¸°
3. "ê·¸ëŸ°ë°", "ì§„í–‰ë¥ ", "ìƒíƒœ" ê°™ì€ ì–´ìƒ‰í•œ í‘œí˜„ í”¼í•˜ê¸°
4. ì „ì²´ ì‘ë‹µì´ í•˜ë‚˜ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ì²˜ëŸ¼ ëŠê»´ì§€ë„ë¡
5. ë‹¤ìŒ í•„ìš”í•œ ì •ë³´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë¬¼ì–´ë³´ê¸°

ì˜ˆì‹œ:
ì‚¬ìš©ì: "í”¼ê³¤í•´" (ì£¼ì œê°€ ì´ë¯¸ íŒŒì´ì¬ìœ¼ë¡œ ì •í•´ì§„ ìƒíƒœ)
ì‘ë‹µ: "ìˆ˜ê³  ë§ìœ¼ì…¨ì–´ìš”! ğŸ˜Š ì˜¤ëŠ˜ í•˜ë£¨ ì •ë§ ê³ ìƒí•˜ì…¨ë„¤ìš”. íœ´ì‹ë„ ì¤‘ìš”í•˜ì§€ë§Œ, íŒŒì´ì¬ ê²½í—˜ì´ ì–´ëŠ ì •ë„ ìˆìœ¼ì‹ ì§€ ê¶ê¸ˆí•´ìš”!"

ì‚¬ìš©ì: "ì•„ë‹ˆ..." (ì£¼ì œê°€ ì´ë¯¸ íŒŒì´ì¬ìœ¼ë¡œ ì •í•´ì§„ ìƒíƒœ)
ì‘ë‹µ: "ê·¸ë ‡êµ°ìš”! ğŸ˜Š ê´œì°®ì•„ìš”. ê·¸ëŸ°ë° íŒŒì´ì¬ ê²½í—˜ì´ ì–´ëŠ ì •ë„ ìˆìœ¼ì‹ ê°€ìš”? ì²˜ìŒ ì‹œì‘í•˜ì‹œëŠ” ê±´ê°€ìš”, ì•„ë‹ˆë©´ ì¡°ê¸ˆ í•´ë³´ì…¨ë‚˜ìš”?"

ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ í•˜ë‚˜ì˜ ì™„ì „í•œ ì‘ë‹µì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”."""

            from langchain_core.messages import HumanMessage, SystemMessage

            messages = [
                SystemMessage(content=f"ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í•™ìŠµ ë©˜í† ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ í•™ìŠµ ì£¼ì œëŠ” ì´ë¯¸ '{topic}'ë¡œ ì •í•´ì ¸ ìˆìœ¼ë¯€ë¡œ, ë°˜ë“œì‹œ {topic}ì— ëŒ€í•œ ì •ë³´ë§Œ ë¬¼ì–´ë³´ì„¸ìš”. ë‹¤ë¥¸ ì£¼ì œëŠ” ì ˆëŒ€ ë¬»ì§€ ë§ˆì„¸ìš”."),
                HumanMessage(content=integrated_prompt)
            ]

            # í†µí•© ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
            integrated_response = ""
            async for chunk in self.llm.astream(messages):
                if hasattr(chunk, 'content') and chunk.content:
                    integrated_response += chunk.content
                    print(chunk.content, end="", flush=True)
                    yield {"type": "message", "content": chunk.content, "node": "integrated_chat"}

            # ê°„ë‹¨í•œ ì •ë³´ ì¶”ê°€ (í•„ìš”í•œ ê²½ìš°ë§Œ)
            if integrated_response:
                final_response = integrated_response.strip()

                # ì´ë¯¸ ìˆ˜ì§‘ëœ ì •ë³´ê°€ ìˆìœ¼ë©´ ê°„ë‹¨íˆ í‘œì‹œ
                if topic or constraints or goal:
                    simple_info = "\n\nğŸ“ **í˜„ì¬ê¹Œì§€:**"
                    if topic:
                        simple_info += f" ì£¼ì œ({topic})"
                    if constraints:
                        simple_info += f" ìˆ˜ì¤€ íŒŒì•…ë¨"
                    if goal:
                        simple_info += f" ëª©í‘œ({goal})"

                    final_response += simple_info

                # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
                self.conversation_history.append({
                    "role": "assistant",
                    "content": final_response
                })

        except Exception as e:
            print(f"âŒ ìˆœì°¨ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            yield {"type": "error", "content": f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}

    async def _get_profiling_status(self) -> dict:
        """í˜„ì¬ í”„ë¡œíŒŒì¼ë§ ìƒíƒœ í™•ì¸"""
        try:
            if not self.current_session_id:
                return {"in_progress": False, "missing_step": None, "completion_rate": 0}

            from servers.user_assessment import load_session
            session_data = load_session(self.current_session_id)

            if not session_data:
                return {"in_progress": False, "missing_step": None, "completion_rate": 0}

            topic = session_data.get("topic", "").strip()
            constraints = session_data.get("constraints", "").strip()
            goal = session_data.get("goal", "").strip()

            topic_complete = bool(topic)
            # ì œì•½ì¡°ê±´ì€ ìˆ˜ì¤€ë§Œ ìˆì–´ë„ ì™„ë£Œë¡œ ê°„ì£¼ (ì‹œê°„ ì •ë³´ëŠ” ì„ íƒì‚¬í•­)
            constraints_complete = bool(constraints and any(kw in constraints for kw in ["ì´ˆë³´", "ì¤‘ê¸‰", "ê³ ê¸‰", "ìˆ˜ì¤€", "ê²½í—˜", "ì²˜ìŒ", "ì…ë¬¸", "ê¸°ì´ˆ"]))
            goal_complete = bool(goal)

            completed_steps = []
            if topic_complete: completed_steps.append("topic")
            if constraints_complete: completed_steps.append("constraints")
            if goal_complete: completed_steps.append("goal")

            completion_rate = len(completed_steps) / 3.0

            # ë‹¤ìŒ í•„ìš”í•œ ë‹¨ê³„ ê²°ì •
            missing_step = None
            if not topic_complete:
                missing_step = "topic"
            elif not constraints_complete:
                missing_step = "constraints"
            elif not goal_complete:
                missing_step = "goal"

            is_in_progress = not (topic_complete and constraints_complete and goal_complete)

            return {
                "in_progress": is_in_progress,
                "missing_step": missing_step,
                "completion_rate": completion_rate,
                "completed_steps": completed_steps,
                "topic": topic,
                "constraints": constraints,
                "goal": goal
            }

        except Exception as e:
            print(f"âŒ í”„ë¡œíŒŒì¼ë§ ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {e}")
            return {"in_progress": False, "missing_step": None, "completion_rate": 0}



    async def _handle_unified_conversation(self, message: str) -> AsyncGenerator[dict, None]:
        """ë¶„ë¥˜ ê¸°ë°˜ ëŒ€í™” ì²˜ë¦¬ - with_structured_outputìœ¼ë¡œ ëª…í™•í•œ ì•¡ì…˜ ì„ íƒ"""
        print(f"ğŸ¤– ë¶„ë¥˜ ê¸°ë°˜ ëŒ€í™” ì²˜ë¦¬ ì‹œì‘")
        
        try:
            # 1. ì‚¬ìš©ì ì˜ë„ ë¶„ë¥˜
            classification = await self._classify_user_intent(message)
            
            # 2. ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¼ ì²˜ë¦¬
            if classification.action == ActionType.USER_PROFILING:
                async for chunk in self._handle_user_profiling(message):
                    yield chunk
                    
            elif classification.action == ActionType.GENERATE_CURRICULUM:
                async for chunk in self._handle_curriculum_generation(message):
                    yield chunk

            elif classification.action == ActionType.PROFILING_GENERAL_CHAT:
                # ìˆœì°¨ì  ì²˜ë¦¬: General Chat ì‘ë‹µ â†’ ì—°ê²°ì–´ â†’ í”„ë¡œíŒŒì¼ë§
                async for chunk in self._handle_profiling_general_chat_sequential(message):
                    yield chunk

            else:  # GENERAL_CHAT
                async for chunk in self._handle_general_chat(message):
                    yield chunk
                
        except Exception as e:
            print(f"âŒ í†µí•© ëŒ€í™” ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            yield {
                "type": "error",
                "content": f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }
    
    def clear_conversation(self):
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        # ê¸°ë³¸ ì¸ì‚¬ë§ë¡œ ì¬ì„¤ì •
        self.conversation_history = [
            {
                "role": "assistant",
                "content": "ì•ˆë…•í•˜ì„¸ìš”! LearnAI ì…ë‹ˆë‹¤. ì–´ë–¤ ì£¼ì œì— ëŒ€í•´ ë°°ìš°ê³  ì‹¶ìœ¼ì‹ ì§€ ì•Œë ¤ì£¼ì‹œë©´ ë§ì¶¤í˜• í•™ìŠµ ê³„íšì„ í•¨ê»˜ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤!"
            }
        ]
        # ì„¸ì…˜ IDëŠ” main.pyì—ì„œ ì„¤ì •í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì´ˆê¸°í™”í•˜ì§€ ì•ŠìŒ
        print("ğŸ’¬ ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def _extract_content(self, content) -> Optional[str]:
        """ë©”ì‹œì§€ ë‚´ìš© ì¶”ì¶œ í—¬í¼ í•¨ìˆ˜"""
        if isinstance(content, str):
            return content
        elif isinstance(content, list):
            for item in content:
                if isinstance(item, dict) and "text" in item:
                    return item["text"]
        return None


# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
MultiAgentSystem = MultiMCPAgent
