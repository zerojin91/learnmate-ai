"""
Resource Collector Agent - í•™ìŠµ ë¦¬ì†ŒìŠ¤ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
"""
from typing import List, Dict, Any
import asyncio
import httpx
from bs4 import BeautifulSoup
import re
from urllib.parse import quote
import sys
import traceback

from .base_agent import BaseAgent
from .state import CurriculumState, ProcessingPhase


class ResourceCollectorAgent(BaseAgent):
    """í•™ìŠµ ë¦¬ì†ŒìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ì—ì´ì „íŠ¸"""

    async def execute(self, state: CurriculumState) -> CurriculumState:
        """ë¦¬ì†ŒìŠ¤ ìˆ˜ì§‘ ì‹¤í–‰"""
        try:
            self.safe_update_phase(state, ProcessingPhase.RESOURCE_COLLECTION, "ğŸ” í•™ìŠµ ë¦¬ì†ŒìŠ¤ ìˆ˜ì§‘ ì¤‘...")

            # ê¸°ë³¸ ë¦¬ì†ŒìŠ¤ ìˆ˜ì§‘
            basic_resources = await self._search_basic_resources(state["topic"])
            state["basic_resources"] = basic_resources

            # ëª¨ë“ˆë³„ ë¦¬ì†ŒìŠ¤ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
            if state["detailed_modules"]:
                module_resources = await self._collect_all_module_resources(
                    state["topic"],
                    state["detailed_modules"]
                )
                state["module_resources"] = module_resources

            self.log_debug(f"Collected {len(basic_resources)} basic resources and module-specific resources")

            return state

        except Exception as e:
            return self.handle_error(state, e, "Resource collection failed")

    async def _search_basic_resources(self, topic: str, num_results: int = 10) -> List[Dict[str, str]]:
        """ê¸°ë³¸ ë¦¬ì†ŒìŠ¤ ê²€ìƒ‰"""
        try:
            search_query = f"{topic} í•™ìŠµ ê°•ì˜ íŠœí† ë¦¬ì–¼"
            return await self._search_web_resources(search_query, num_results)
        except Exception as e:
            self.log_debug(f"Basic resource search failed: {e}")
            return []

    async def _collect_all_module_resources(self, topic: str, modules: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """ëª¨ë“  ëª¨ë“ˆì˜ ë¦¬ì†ŒìŠ¤ë¥¼ ë³‘ë ¬ë¡œ ìˆ˜ì§‘"""

        # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ íƒœìŠ¤í¬ ìƒì„±
        tasks = []
        for module in modules:
            module_topic = f"{topic} {module.get('title', '')}"
            task = self._collect_module_resources(module_topic, module)
            tasks.append(task)

        # ë³‘ë ¬ ì‹¤í–‰
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ê²°ê³¼ ì •ë¦¬
        module_resources = {}
        for i, result in enumerate(results):
            module_key = f"week_{modules[i].get('week', i+1)}"
            if isinstance(result, Exception):
                self.log_debug(f"Module {module_key} resource collection failed: {result}")
                module_resources[module_key] = {"videos": [], "web_links": [], "documents": []}
            else:
                module_resources[module_key] = result

        return module_resources

    async def _collect_module_resources(self, module_topic: str, module: Dict[str, Any]) -> Dict[str, List[Dict[str, Any]]]:
        """ê°œë³„ ëª¨ë“ˆì˜ ë¦¬ì†ŒìŠ¤ ìˆ˜ì§‘"""
        week_title = module.get('title', '')

        try:
            # ë³‘ë ¬ë¡œ ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ë¦¬ì†ŒìŠ¤ ìˆ˜ì§‘
            tasks = [
                self._search_kmooc_resources(module_topic, week_title),
                self._search_document_resources(module_topic, week_title),  # Pinecone ê²€ìƒ‰ í¬í•¨
                self._search_web_resources(f"{module_topic} ê°•ì˜", 5)
            ]

            kmooc_results, doc_results, web_results = await asyncio.gather(*tasks, return_exceptions=True)

            # ê²°ê³¼ ì •ë¦¬ (ì›ë³¸ ì½”ë“œ í˜•ì‹ê³¼ ë™ì¼)
            resources = {
                "videos": kmooc_results if not isinstance(kmooc_results, Exception) else [],
                "web_links": web_results if not isinstance(web_results, Exception) else [],
                "documents": doc_results if not isinstance(doc_results, Exception) else [],
                "total_resources": 0,
                "resources_with_content": 0,
                "content_coverage": 0.0
            }

            # í†µê³„ ì •ë³´ ê³„ì‚°
            total_resources = len(resources["videos"]) + len(resources["web_links"]) + len(resources["documents"])
            resources_with_content = (
                len(resources["videos"]) +  # ë¹„ë””ì˜¤ëŠ” í•­ìƒ ì½˜í…ì¸ ê°€ ìˆë‹¤ê³  ê°€ì •
                len([r for r in resources["web_links"] if r.get('content')]) +
                len([d for d in resources["documents"] if d.get('has_content', False)])
            )

            resources["total_resources"] = total_resources
            resources["resources_with_content"] = resources_with_content
            resources["content_coverage"] = resources_with_content / max(total_resources, 1)

            return resources

        except Exception as e:
            self.log_debug(f"Module resource collection failed for {week_title}: {e}")
            return {"videos": [], "documents": [], "web_links": []}

    async def _search_kmooc_resources(self, topic: str, week_title: str = None, top_k: int = 5) -> List[Dict[str, Any]]:
        """K-MOOC DBì—ì„œ ê´€ë ¨ ì˜ìƒì„ ê²€ìƒ‰í•©ë‹ˆë‹¤ (Pinecone API ì‚¬ìš©)"""
        try:
            # Pinecone ê²€ìƒ‰ API í˜¸ì¶œ
            search_query = f"{topic}"
            if week_title:
                search_query += f" {week_title}"

            search_payload = {
                "query": search_query,
                "top_k": top_k,
                "namespace": "kmooc_engineering",
                "filter": {"institution": {"$ne": ""}},
                "rerank": True,
                "include_metadata": True
            }

            print(f"DEBUG: K-MOOC ê²€ìƒ‰ ì‹œì‘ - query: {search_query}", file=sys.stderr, flush=True)

            # pinecone_search_kmooc.py ì„œë²„ê°€ localhost:8099ì—ì„œ ì‹¤í–‰ ì¤‘ì´ë¼ê³  ê°€ì •
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    "http://localhost:8099/search",
                    json=search_payload,
                    timeout=10.0
                )

            if response.status_code == 200:
                result = response.json()
                kmooc_videos = []

                print(f"DEBUG: K-MOOC ê²€ìƒ‰ ì‘ë‹µ - ê²°ê³¼ ìˆ˜: {len(result.get('results', []))}", file=sys.stderr, flush=True)

                for item in result.get("results", []):
                    metadata = item.get("metadata", {})
                    if metadata:
                        # Summary íŒŒì‹±í•˜ì—¬ ê°•ì¢Œ ì •ë³´ ì¶”ì¶œ
                        summary = metadata.get("summary", "")
                        parsed_info = self._parse_kmooc_summary(summary)

                        # ì œëª© ê²°ì •: íŒŒì‹±ëœ ì œëª© > ê¸°ë³¸ "K-MOOC ê°•ì¢Œ"
                        course_title = parsed_info.get("title") or "K-MOOC ê°•ì¢Œ"

                        # ì„¤ëª… ê²°ì •: íŒŒì‹±ëœ ì„¤ëª… > ì£¼ìš” ë‚´ìš© > ê°•ì¢Œ ëª©í‘œ > ê¸°ë³¸ ë©”ì‹œì§€
                        description = (
                            parsed_info.get("description") or
                            parsed_info.get("main_content") or
                            parsed_info.get("course_goal") or
                            "K-MOOC ì˜¨ë¼ì¸ ê°•ì¢Œ"
                        )

                        video_info = {
                            "title": course_title,
                            "description": description,
                            "url": metadata.get("url", ""),
                            "institution": metadata.get("institution", "").replace(" ìš´ì˜ê¸°ê´€ ë°”ë¡œê°€ê¸°ìƒˆì°½ì—´ë¦¼", ""),
                            "course_goal": parsed_info.get("course_goal", ""),
                            "duration": parsed_info.get("duration", ""),
                            "difficulty": parsed_info.get("difficulty", ""),
                            "class_time": parsed_info.get("class_time", ""),
                            "score": item.get("score", 0.0),
                            "source": "K-MOOC"
                        }
                        kmooc_videos.append(video_info)

                print(f"DEBUG: K-MOOC ìµœì¢… ë¹„ë””ì˜¤ ìˆ˜: {len(kmooc_videos)}", file=sys.stderr, flush=True)
                return kmooc_videos

            else:
                print(f"DEBUG: K-MOOC ê²€ìƒ‰ ì‹¤íŒ¨ - ìƒíƒœì½”ë“œ: {response.status_code}", file=sys.stderr, flush=True)
                return []

        except Exception as e:
            print(f"DEBUG: K-MOOC ê²€ìƒ‰ ì˜¤ë¥˜: {type(e).__name__}: {e}", file=sys.stderr, flush=True)
            print(f"DEBUG: ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}", file=sys.stderr, flush=True)
            return []

    def _parse_kmooc_summary(self, summary: str) -> Dict[str, str]:
        """K-MOOC summaryì—ì„œ ê°•ì¢Œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤"""
        try:
            if not summary:
                return {}

            parsed_info = {}

            # ê°•ì¢Œ ëª©í‘œ ì¶”ì¶œ
            goal_match = re.search(r'\*\*ê°•ì¢Œ ëª©í‘œ:\*\*\s*([^\n*]+)', summary)
            if goal_match:
                parsed_info["course_goal"] = goal_match.group(1).strip()
                # ê°•ì¢Œ ëª©í‘œì—ì„œ ì²« ë²ˆì§¸ ë¬¸ì¥ì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš©
                goal_text = goal_match.group(1).strip()
                # ì²« ë²ˆì§¸ ë¬¸ì¥ì´ë‚˜ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì œëª©ìœ¼ë¡œ ì¶”ì¶œ
                if "," in goal_text:
                    parsed_info["title"] = goal_text.split(",")[0].strip()
                else:
                    parsed_info["title"] = goal_text[:50] + "..." if len(goal_text) > 50 else goal_text

            # ì£¼ìš” ë‚´ìš© ì¶”ì¶œ
            content_match = re.search(r'\*\*ì£¼ìš” ë‚´ìš©:\*\*\s*([^\n*]+)', summary)
            if content_match:
                content = content_match.group(1).strip()
                parsed_info["main_content"] = content
                # ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ ì„¤ëª…ìœ¼ë¡œ ì‚¬ìš©
                if len(content) > 100:
                    parsed_info["description"] = content[:97] + "..."
                else:
                    parsed_info["description"] = content

            # ê°•ì¢Œ ê¸°ê°„ ì¶”ì¶œ
            duration_match = re.search(r'\*\*ê°•ì¢Œ ê¸°ê°„:\*\*[^()]*\((\d+ì£¼)\)', summary)
            if duration_match:
                parsed_info["duration"] = duration_match.group(1)

            # ë‚œì´ë„ ì¶”ì¶œ
            difficulty_match = re.search(r'\*\*ë‚œì´ë„:\*\*\s*([^\n*]+)', summary)
            if difficulty_match:
                parsed_info["difficulty"] = difficulty_match.group(1).strip()

            # ìˆ˜ì—… ì‹œê°„ ì¶”ì¶œ
            time_match = re.search(r'\*\*ìˆ˜ì—… ì‹œê°„:\*\*[^()]*ì•½\s*([^\n*()]+)', summary)
            if time_match:
                parsed_info["class_time"] = time_match.group(1).strip()

            print(f"DEBUG: Parsed summary - title: {parsed_info.get('title', 'N/A')}, description: {parsed_info.get('description', 'N/A')[:50]}...", file=sys.stderr, flush=True)

            return parsed_info

        except Exception as e:
            print(f"DEBUG: Summary parsing failed: {e}", file=sys.stderr, flush=True)
            return {}

    async def _search_document_resources(self, topic: str, week_title: str = None, top_k: int = 3) -> List[Dict[str, Any]]:
        """Pinecone DBì™€ ì›¹ì—ì„œ ë³‘ë ¬ë¡œ ë¬¸ì„œ ìë£Œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤"""
        # ë³‘ë ¬ë¡œ Pineconeê³¼ ì›¹ ê²€ìƒ‰ ì‹¤í–‰
        tasks = [
            self._search_pinecone_documents(topic, week_title, top_k),
            self._search_web_documents(topic, week_title, top_k)
        ]

        pinecone_results, web_results = await asyncio.gather(*tasks, return_exceptions=True)

        # ê²°ê³¼ ë³‘í•©
        all_documents = []

        # Pinecone ê²°ê³¼ ì¶”ê°€
        if not isinstance(pinecone_results, Exception):
            all_documents.extend(pinecone_results)
            print(f"DEBUG: Pineconeì—ì„œ {len(pinecone_results)}ê°œ ë¬¸ì„œ ì¶”ê°€", file=sys.stderr, flush=True)
        else:
            print(f"DEBUG: Pinecone ê²€ìƒ‰ ì‹¤íŒ¨: {pinecone_results}", file=sys.stderr, flush=True)

        # ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
        if not isinstance(web_results, Exception):
            all_documents.extend(web_results)
            print(f"DEBUG: ì›¹ì—ì„œ {len(web_results)}ê°œ ë¬¸ì„œ ì¶”ê°€", file=sys.stderr, flush=True)
        else:
            print(f"DEBUG: ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {web_results}", file=sys.stderr, flush=True)

        # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        unique_docs = {}
        for doc in all_documents:
            # titleê³¼ sourceë¥¼ í‚¤ë¡œ ì‚¬ìš©í•´ ì¤‘ë³µ ì œê±°
            key = f"{doc.get('title', '')}_{doc.get('source', '')}"
            if key not in unique_docs or doc.get('score', 0) > unique_docs[key].get('score', 0):
                unique_docs[key] = doc

        # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ê³  top_kê°œë§Œ ë°˜í™˜
        sorted_docs = sorted(unique_docs.values(),
                           key=lambda x: x.get('score', 0),
                           reverse=True)

        return sorted_docs[:top_k*2]  # ë³‘ë ¬ ê²€ìƒ‰ì´ë¯€ë¡œ ì¢€ ë” ë§ì€ ê²°ê³¼ ë°˜í™˜

    async def _search_pinecone_documents(self, topic: str, week_title: str = None, top_k: int = 3) -> List[Dict[str, Any]]:
        """Pinecone DBì—ì„œ ê´€ë ¨ PDF/ë¬¸ì„œ ìë£Œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤"""
        try:
            # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
            search_query = f"{topic}"
            if week_title:
                search_query += f" {week_title}"

            search_payload = {
                "query": search_query,
                "top_k": top_k,
                "namespace": "main",  # DEFAULT_NAMESPACE ì‚¬ìš©
                "rerank": True,
                "include_metadata": True
            }

            print(f"DEBUG: Pinecone ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘ - query: {search_query}", file=sys.stderr, flush=True)

            # pinecone_search_document.py ì„œë²„ í˜¸ì¶œ
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    "http://localhost:8091/search",
                    json=search_payload,
                    timeout=10.0
                )

            if response.status_code == 200:
                result = response.json()
                documents = []

                print(f"DEBUG: Pinecone ë¬¸ì„œ ê²€ìƒ‰ ì‘ë‹µ - ê²°ê³¼ ìˆ˜: {len(result.get('results', []))}", file=sys.stderr, flush=True)

                for item in result.get("results", []):
                    metadata = item.get("metadata", {})
                    score = item.get("score", 0.0)

                    if metadata and score > 0.5:  # ê´€ë ¨ì„± ì„ê³„ê°’
                        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì •ë³´ ì¶”ì¶œ
                        preview = metadata.get("preview", "").strip()
                        file_path = metadata.get("file_path", "").strip()
                        folder = metadata.get("folder", "").strip()
                        subdir = metadata.get("subdir", "").strip()
                        page_num = metadata.get("page", "")
                        file_sha1 = metadata.get("file_sha1", "")

                        # íŒŒì¼ëª…ì—ì„œ ì œëª© ì¶”ì¶œ
                        doc_title = "PDF ë¬¸ì„œ"
                        if file_path:
                            # íŒŒì¼ ê²½ë¡œì—ì„œ íŒŒì¼ëª…ë§Œ ì¶”ì¶œ
                            filename = file_path.split("/")[-1] if "/" in file_path else file_path
                            # í™•ì¥ì ì œê±°
                            if filename.endswith('.pdf'):
                                filename = filename[:-4]
                            doc_title = filename

                        # ì¹´í…Œê³ ë¦¬ ì •ë³´ (folder ë˜ëŠ” subdir ì‚¬ìš©)
                        category = folder or subdir or "ê¸°íƒ€"

                        # previewê°€ ìˆìœ¼ë©´ ì´ë¥¼ ì£¼ ì½˜í…ì¸ ë¡œ ì‚¬ìš©
                        doc_content = preview if preview else ""

                        # ì„¤ëª… ìƒì„± (preview ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
                        description = preview[:300] + "..." if preview else "ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸° ì—†ìŒ"

                        # ì†ŒìŠ¤ ì •ë³´ êµ¬ì„±
                        source_info = f"{category}/{filename}" if category != "ê¸°íƒ€" else filename

                        documents.append({
                            "title": doc_title,
                            "description": description,
                            "content": doc_content[:2000],  # preview ë‚´ìš© í™•ì¥
                            "preview": preview,  # ì›ë³¸ preview ì €ì¥
                            "source": source_info,
                            "category": category,
                            "file_path": file_path,
                            "file_sha1": file_sha1,
                            "page": page_num,
                            "score": score,
                            "type": "document",
                            "has_content": True if preview else False
                        })

                        print(f"DEBUG: Pinecone ë¬¸ì„œ ì¶”ê°€ - {doc_title[:30]}... (ì ìˆ˜: {score:.3f}, ì¹´í…Œê³ ë¦¬: {category})", file=sys.stderr, flush=True)

                print(f"DEBUG: Pinecone ìµœì¢… ë¬¸ì„œ ìˆ˜: {len(documents)}", file=sys.stderr, flush=True)
                return documents

            else:
                print(f"DEBUG: Pinecone ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨ - ìƒíƒœì½”ë“œ: {response.status_code}", file=sys.stderr, flush=True)
                return []

        except Exception as e:
            print(f"DEBUG: Pinecone ë¬¸ì„œ ê²€ìƒ‰ ì˜¤ë¥˜: {type(e).__name__}: {e}", file=sys.stderr, flush=True)
            print(f"DEBUG: ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}", file=sys.stderr, flush=True)
            return []

    async def _search_web_documents(self, topic: str, week_title: str = None, top_k: int = 3) -> List[Dict[str, Any]]:
        """ì›¹ì—ì„œ ë¬¸ì„œ ìë£Œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤"""
        try:
            search_term = f"{topic} {week_title} ë¬¸ì„œ PDF" if week_title else f"{topic} ë¬¸ì„œ PDF"
            return await self._search_web_resources(search_term, top_k, filter_docs=True)
        except Exception as e:
            print(f"DEBUG: ì›¹ ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}", file=sys.stderr, flush=True)
            return []

    async def _search_web_resources(self, query: str, num_results: int = 10, filter_docs: bool = False) -> List[Dict[str, str]]:
        """ì›¹ ë¦¬ì†ŒìŠ¤ ê²€ìƒ‰"""
        try:
            encoded_query = quote(query)
            search_url = f"https://search.naver.com/search.naver?query={encoded_query}"

            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(search_url)
                if response.status_code != 200:
                    return []

                soup = BeautifulSoup(response.text, 'html.parser')

                # ë‹¤ì–‘í•œ ë§í¬ íŒ¨í„´ ì‹œë„
                link_patterns = [
                    r'<a[^>]*href="([^"]*)"[^>]*class="[^"]*link[^"]*"[^>]*>([^<]*)</a>',
                    r'<a[^>]*class="[^"]*result[^"]*"[^>]*href="([^"]*)"[^>]*>([^<]*)</a>',
                    r'<a[^>]*href="([^"]*)"[^>]*>([^<]*)</a>'
                ]

                resources = []
                for pattern in link_patterns:
                    matches = re.findall(pattern, response.text, re.IGNORECASE)

                    for url, title in matches[:num_results]:
                        if not url or not title:
                            continue

                        title = re.sub(r'<[^>]+>', '', title).strip()

                        if filter_docs and not any(ext in url.lower() for ext in ['.pdf', '.doc', '.ppt']):
                            continue

                        if len(title) > 5 and url.startswith('http'):
                            resources.append({
                                "title": title,
                                "url": url,
                                "source": "Web Search"
                            })

                    if resources:
                        break

                return resources[:num_results]

        except Exception as e:
            self.log_debug(f"Web search failed: {e}")
            return []