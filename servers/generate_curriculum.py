from mcp.server.fastmcp import FastMCP
import asyncio
import httpx
from typing import List, Dict, Any, Optional
from datetime import datetime
import json
import re
from urllib.parse import quote
import os
import glob
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from pydantic import BaseModel, Field
from enum import Enum
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import Config

# MCP ì„œë²„ ì„¤ì •
mcp = FastMCP(
    "CurriculumGenerator",
    instructions="Generate personalized learning curriculums from session data",
    host="0.0.0.0",
    port=8006,
)

# LLM ì´ˆê¸°í™”
def initialize_llm():
    try:
        llm = ChatOpenAI(
            base_url=Config.LLM_BASE_URL,
            api_key=Config.LLM_API_KEY,
            model=Config.LLM_MODEL,
            temperature=0.7,
            max_tokens=Config.LLM_MAX_TOKENS,
        )
        print(f"ğŸ¤– LLM initialized: {Config.LLM_MODEL}")
        return llm, True
    except Exception as e:
        print(f"âŒ LLM initialization failed: {e}")
        return None, False

llm, llm_available = initialize_llm()

# Pydantic ëª¨ë¸ ì •ì˜
class LevelEnum(str, Enum):
    BEGINNER = "beginner"
    INTERMEDIATE = "intermediate"
    ADVANCED = "advanced"

class SessionParameters(BaseModel):
    """ì„¸ì…˜ íŒŒë¼ë¯¸í„° ì¶”ì¶œì„ ìœ„í•œ êµ¬ì¡°í™”ëœ ì¶œë ¥"""
    level: LevelEnum = Field(description="í•™ìŠµìì˜ ë ˆë²¨ (beginner/intermediate/advanced)")
    duration_weeks: int = Field(description="í•™ìŠµ ê¸°ê°„ (ì£¼ ë‹¨ìœ„, 1-12 ì‚¬ì´)", ge=1, le=12)
    focus_areas: List[str] = Field(description="í•™ìŠµ í¬ì»¤ìŠ¤ ì˜ì—­ë“¤")
    
class ExtractionRequest(BaseModel):
    """íŒŒë¼ë¯¸í„° ì¶”ì¶œ ìš”ì²­"""
    constraints: str = Field(description="í•™ìŠµ ì œì•½ ì¡°ê±´")
    goal: str = Field(description="í•™ìŠµ ëª©í‘œ")

# ë°ì´í„°ë² ì´ìŠ¤ í´ë˜ìŠ¤
class CurriculumDB:
    def __init__(self, data_dir: str = "data"):
        self.curriculums = {}
        self.data_dir = data_dir
        os.makedirs(data_dir, exist_ok=True)
        self._load_data()
    
    def _load_data(self):
        try:
            file_path = os.path.join(self.data_dir, "curriculums.json")
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    self.curriculums = json.load(f)
                print(f"ğŸ“š Loaded {len(self.curriculums)} curriculums")
        except Exception as e:
            print(f"âŒ Error loading data: {e}")
            self.curriculums = {}
    
    def _save_data(self):
        try:
            file_path = os.path.join(self.data_dir, "curriculums.json")
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(self.curriculums, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âŒ Error saving data: {e}")
    
    def save_curriculum(self, user_id: str, curriculum: Dict):
        if user_id not in self.curriculums:
            self.curriculums[user_id] = []
        curriculum['id'] = len(self.curriculums[user_id])
        curriculum['created_at'] = datetime.now().isoformat()
        self.curriculums[user_id].append(curriculum)
        self._save_data()
        return curriculum['id']
    
    def get_curriculum(self, user_id: str, curriculum_id: int) -> Optional[Dict]:
        if user_id in self.curriculums and curriculum_id < len(self.curriculums[user_id]):
            return self.curriculums[user_id][curriculum_id]
        return None

db = CurriculumDB()

# ì„¸ì…˜ ë°ì´í„° ë¡œë”
class SessionLoader:
    def __init__(self, sessions_dir: str = "sessions"):
        self.sessions_dir = sessions_dir
    
    def get_completed_sessions(self) -> List[Dict[str, Any]]:
        completed_sessions = []
        session_files = glob.glob(os.path.join(self.sessions_dir, "*.json"))
        
        for session_file in session_files:
            try:
                with open(session_file, 'r', encoding='utf-8') as f:
                    session_data = json.load(f)
                
                if (session_data.get("completed") and 
                    session_data.get("topic") and 
                    session_data.get("constraints") and 
                    session_data.get("goal")):
                    
                    completed_sessions.append({
                        "session_id": session_data.get("session_id"),
                        "topic": session_data.get("topic"),
                        "constraints": session_data.get("constraints"),
                        "goal": session_data.get("goal")
                    })
            except Exception as e:
                print(f"âš ï¸ Error loading {session_file}: {e}")
                continue
        
        return completed_sessions
    
    async def extract_parameters_with_llm(self, constraints: str, goal: str, max_retries: int = 3) -> Dict[str, Any]:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì„¸ì…˜ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ í¬í•¨)"""
        if not llm_available:
            print("âš ï¸ LLM not available, using fallback method")
            return self.parse_constraints_fallback(constraints, goal)
        
        for attempt in range(max_retries):
            try:
                prompt = f"""ë‹¤ìŒ í•œêµ­ì–´ í•™ìŠµ ì œì•½ì¡°ê±´ê³¼ ëª©í‘œì—ì„œ í•™ìŠµ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

ì œì•½ì¡°ê±´: "{constraints}"
ëª©í‘œ: "{goal}"

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

1. ë ˆë²¨ (level):
   - "ì´ˆë³´", "ì²˜ìŒ", "ì‹œì‘", "beginner" â†’ "beginner"
   - "ì¤‘ê¸‰", "ì–´ëŠì •ë„", "ì¤‘ê°„", "intermediate" â†’ "intermediate"  
   - "ê³ ê¸‰", "ì „ë¬¸", "ìˆ™ë ¨", "advanced" â†’ "advanced"

2. ê¸°ê°„ (duration_weeks):
   - "1ì£¼", "ì¼ì£¼ì¼", "1week" â†’ 1
   - "2ì£¼", "2week", "2ì¼" â†’ 2
   - "1ë‹¬", "í•œë‹¬", "1month" â†’ 4
   - "2ë‹¬", "ë‘ë‹¬", "2month" â†’ 8
   - "3ë‹¬", "ì„¸ë‹¬", "3month" â†’ 12
   - ëª…ì‹œë˜ì§€ ì•Šìœ¼ë©´ â†’ 4

3. í¬ì»¤ìŠ¤ ì˜ì—­ (focus_areas):
   - "ì›¹", "web" â†’ ["web development"]
   - "ë°ì´í„°", "data" â†’ ["data analysis"]
   - "ë¨¸ì‹ ëŸ¬ë‹", "AI", "ì¸ê³µì§€ëŠ¥" â†’ ["machine learning"]
   - "ì•±", "ëª¨ë°”ì¼", "app" â†’ ["mobile development"]
   - "ê°œì¸ í”„ë¡œì íŠ¸", "í† ì´ í”„ë¡œì íŠ¸" â†’ ["personal projects"]
   - "ì·¨ì—…", "ë©´ì ‘", "job" â†’ ["job preparation"]
   - ê¸°íƒ€ ê´€ë ¨ í‚¤ì›Œë“œë“¤ì„ í¬í•¨

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
                
                # Pydanticì„ ì‚¬ìš©í•œ êµ¬ì¡°í™”ëœ ì¶œë ¥
                structured_llm = llm.with_structured_output(SessionParameters)
                
                messages = [
                    SystemMessage(content="ë‹¹ì‹ ì€ í•œêµ­ì–´ í•™ìŠµ ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
                    HumanMessage(content=prompt)
                ]
                
                print(f"ğŸ¤– LLM parameter extraction attempt {attempt + 1}/{max_retries}...")
                result = await structured_llm.ainvoke(messages)
                
                # ì„±ê³µí•˜ë©´ ê²°ê³¼ ë°˜í™˜
                extracted_params = {
                    "level": result.level.value,
                    "duration_weeks": result.duration_weeks,
                    "focus_areas": result.focus_areas
                }
                print(f"âœ… LLM extraction successful on attempt {attempt + 1}")
                return extracted_params
            
            except Exception as e:
                print(f"âš ï¸ LLM extraction attempt {attempt + 1} failed: {e}")
                
                # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ê³„ì† ì¬ì‹œë„
                if attempt < max_retries - 1:
                    print(f"ğŸ”„ Retrying... ({max_retries - attempt - 1} attempts remaining)")
                    continue
                else:
                    print(f"âŒ All {max_retries} LLM attempts failed, falling back to rule-based parsing")
                    break
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ì‹œ fallback ì‚¬ìš©
        return self.parse_constraints_fallback(constraints, goal)
    
    def parse_constraints_fallback(self, constraints: str, goal: str) -> Dict[str, Any]:
        """LLM ì‹¤íŒ¨ì‹œ ê¸°ì¡´ ê·œì¹™ ê¸°ë°˜ íŒŒì‹± (fallback)"""
        constraints_lower = constraints.lower()
        goal_lower = goal.lower()
        
        # ë ˆë²¨ íŒŒì‹±
        level = "beginner"
        if any(word in constraints_lower for word in ["ì¤‘ê¸‰", "ì–´ëŠì •ë„", "intermediate"]):
            level = "intermediate"
        elif any(word in constraints_lower for word in ["ê³ ê¸‰", "ì „ë¬¸", "advanced"]):
            level = "advanced"
        
        # ê¸°ê°„ íŒŒì‹± (ì£¼ ë‹¨ìœ„)
        duration_weeks = 4  # ê¸°ë³¸ê°’
        if any(word in constraints_lower for word in ["1ì£¼", "1week"]):
            duration_weeks = 1
        elif any(word in constraints_lower for word in ["2ì£¼", "2week", "2ì¼"]):
            duration_weeks = 2
        elif any(word in constraints_lower for word in ["1ë‹¬", "1month"]):
            duration_weeks = 4
        elif any(word in constraints_lower for word in ["2ë‹¬", "2month"]):
            duration_weeks = 8
        
        # í¬ì»¤ìŠ¤ ì˜ì—­ ì¶”ì¶œ
        focus_areas = []
        mappings = {
            "ì›¹": ["web development"],
            "ë°ì´í„°": ["data analysis"],
            "ë¨¸ì‹ ëŸ¬ë‹": ["machine learning"],
            "ì•±": ["mobile development"],
            "ê°œì¸ í”„ë¡œì íŠ¸": ["personal projects"],
            "ì·¨ì—…": ["job preparation"]
        }
        
        for keyword, areas in mappings.items():
            if keyword in goal_lower:
                focus_areas.extend(areas)
        
        return {
            "level": level, 
            "duration_weeks": duration_weeks,
            "focus_areas": list(set(focus_areas))
        }
    
    def update_session_with_curriculum(self, session_id: str, curriculum: Dict[str, Any]) -> bool:
        """ì„¸ì…˜ JSON íŒŒì¼ì— ì»¤ë¦¬í˜ëŸ¼ ì •ë³´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤"""
        try:
            session_file_path = os.path.join(self.sessions_dir, f"{session_id}.json")
            
            if not os.path.exists(session_file_path):
                print(f"âš ï¸ Session file not found: {session_file_path}")
                return False
            
            # ê¸°ì¡´ ì„¸ì…˜ ë°ì´í„° ë¡œë“œ
            with open(session_file_path, 'r', encoding='utf-8') as f:
                session_data = json.load(f)
            
            # ëª¨ë“ˆ ë°ì´í„° ì •ê·œí™” (LLM ì˜¤ë¥˜ ìˆ˜ì •)
            modules = curriculum.get("modules", [])
            cleaned_modules = []
            
            for module in modules:
                cleaned_module = {}
                for key, value in module.items():
                    # í‚¤ ì •ê·œí™”: í•œêµ­ì–´ í‚¤ë¥¼ ì˜ì–´ë¡œ ë§¤í•‘
                    cleaned_key = key.replace("_ ", "_").replace(" _", "_").strip()
                    
                    # í•œêµ­ì–´ í‚¤ ë§¤í•‘
                    key_mapping = {
                        "key_ê°œë…ë“¤": "key_concepts",
                        "key_ê°œë…": "key_concepts",
                        "estimated_ì‹œê°„": "estimated_hours",
                        "estimated_ì‹œê°„": "estimated_hours",
                        "í•™ìŠµëª©í‘œ": "objectives",
                        "ì œëª©": "title",
                        "ì„¤ëª…": "description",
                        "ì£¼ì°¨": "week"
                    }
                    
                    if cleaned_key in key_mapping:
                        cleaned_key = key_mapping[cleaned_key]
                    
                    cleaned_module[cleaned_key] = value
                cleaned_modules.append(cleaned_module)
            
            # ì„¸ë¶€ ì»¤ë¦¬í˜ëŸ¼ ì •ë³´ ì¶”ê°€ (ì „ì²´ ë‚´ìš© í¬í•¨)
            curriculum_full = {
                "curriculum_generated": True,
                "curriculum_id": curriculum.get("curriculum_id"),
                "title": curriculum.get("title"),
                "level": curriculum.get("level"),
                "duration_weeks": curriculum.get("duration_weeks"),
                "overall_goal": curriculum.get("overall_goal"),
                "generated_at": curriculum.get("generated_at"),
                "original_constraints": curriculum.get("original_constraints"),
                "original_goal": curriculum.get("original_goal"),
                
                # ì •ê·œí™”ëœ ëª¨ë“ˆ ì •ë³´ í¬í•¨
                "modules": cleaned_modules,
                "modules_count": len(cleaned_modules),
                
                # í•™ìŠµ ìë£Œ ì •ë³´ í¬í•¨
                "resources": curriculum.get("resources", []),
                "resources_count": len(curriculum.get("resources", [])),
                
                # í†µê³„ ì •ë³´ ê³„ì‚°
                "total_estimated_hours": sum(
                    module.get("estimated_hours", module.get("estimated_ hours", 0)) 
                    for module in cleaned_modules
                ),
                "average_hours_per_week": 0  # ì„ì‹œë¡œ 0ìœ¼ë¡œ ì„¤ì •
            }
            
            # í‰ê·  ì‹œê°„ ì¬ê³„ì‚°
            total_hours = curriculum_full["total_estimated_hours"]
            duration = curriculum.get("duration_weeks", 1)
            curriculum_full["average_hours_per_week"] = (
                total_hours / duration
            ) if duration > 0 else 0
            
            session_data["curriculum"] = curriculum_full
            
            # íŒŒì¼ì— ë‹¤ì‹œ ì €ì¥
            with open(session_file_path, 'w', encoding='utf-8') as f:
                json.dump(session_data, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… Session {session_id} updated with curriculum info")
            return True
            
        except Exception as e:
            print(f"âŒ Failed to update session {session_id}: {e}")
            return False

session_loader = SessionLoader()

# í•™ìŠµ ìë£Œ ê²€ìƒ‰
async def search_resources(topic: str, num_results: int = 10) -> List[Dict[str, str]]:
    try:
        encoded_query = quote(f"{topic} tutorial")
        url = f"https://lite.duckduckgo.com/lite/?q={encoded_query}"
        
        async with httpx.AsyncClient() as client:
            response = await client.get(url, timeout=10.0)
            
            if response.status_code == 200:
                content = response.text
                results = []
                
                link_pattern = r'<a[^>]*class="result-link"[^>]*href="([^"]*)"[^>]*>(.*?)</a>'
                matches = re.finditer(link_pattern, content)
                
                for i, match in enumerate(matches):
                    if len(results) >= num_results:
                        break
                    results.append({
                        "title": re.sub(r'<[^>]+>', '', match.group(2)).strip(),
                        "url": match.group(1)
                    })
                
                return results
    except Exception as e:
        print(f"âŒ Search failed: {e}")
    
    return []

# LLMì„ ì‚¬ìš©í•œ ì»¤ë¦¬í˜ëŸ¼ ìƒì„±
async def generate_with_llm(topic: str, level: str, duration_weeks: int, focus_areas: List[str], resources: List[Dict[str, str]] = None) -> Dict[str, Any]:
    if not llm_available:
        return create_basic_curriculum(topic, level, duration_weeks)
    
    try:
        focus_text = ', '.join(focus_areas) if focus_areas else 'General coverage'
        
        # í•™ìŠµ ìë£Œê°€ ìˆìœ¼ë©´ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
        resources_text = ""
        if resources and len(resources) > 0:
            resources_text = "\n\nAvailable learning resources:\n"
            for i, resource in enumerate(resources[:5], 1):
                resources_text += f"{i}. {resource.get('title', 'No title')} - {resource.get('url', 'No URL')}\n"
            resources_text += "\nConsider these resources when designing the curriculum modules.\n"
        else:
            resources_text = "\n\nNote: No specific learning resources were found, but design a comprehensive curriculum anyway.\n"
        
        prompt = f"""ë‹¤ìŒ ì¡°ê±´ì— ë§ëŠ” {duration_weeks}ì£¼ ì»¤ë¦¬í˜ëŸ¼ì„ ìƒì„±í•´ì£¼ì„¸ìš”:

í•™ìŠµ ì£¼ì œ: {topic}
í•™ìŠµ ë ˆë²¨: {level}
í¬ì»¤ìŠ¤ ì˜ì—­: {focus_text}{resources_text}
ì¤‘ìš”: JSON í‚¤ëŠ” ì˜ì–´ë¡œ, ëª¨ë“  ë‚´ìš©ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”!

ê° ëª¨ë“ˆì€ ë‹¤ìŒì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:
- ëª…í™•í•œ ì œëª©ê³¼ ì„¤ëª… (í•œêµ­ì–´)
- 3-4ê°œì˜ í•™ìŠµ ëª©í‘œ (í•œêµ­ì–´)
- ì˜ˆìƒ í•™ìŠµ ì‹œê°„
- í•µì‹¬ ê°œë…ë“¤ (í•œêµ­ì–´)

JSON í˜•ì‹ (í‚¤ëŠ” ì˜ì–´, ê°’ì€ í•œêµ­ì–´):
{{
    "modules": [
        {{
            "week": 1,
            "title": "ëª¨ë“ˆ ì œëª© (í•œêµ­ì–´)",
            "description": "ëª¨ë“ˆì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª… (í•œêµ­ì–´)",
            "objectives": ["í•™ìŠµëª©í‘œ1 (í•œêµ­ì–´)", "í•™ìŠµëª©í‘œ2 (í•œêµ­ì–´)", "í•™ìŠµëª©í‘œ3 (í•œêµ­ì–´)"],
            "key_concepts": ["í•µì‹¬ê°œë… 1 (í•œêµ­ì–´)", "í•µì‹¬ê°œë… 2 (í•œêµ­ì–´)"],
            "estimated_hours": 10
        }}
    ],
    "overall_goal": "ì „ì²´ í•™ìŠµ ëª©í‘œ (í•œêµ­ì–´)"
}}"""
        
        messages = [
            SystemMessage(content="ë‹¹ì‹ ì€ ì „ë¬¸ ì»¤ë¦¬í˜ëŸ¼ ì„¤ê³„ìì…ë‹ˆë‹¤. ë°˜ë“œì‹œ JSON í‚¤ëŠ” ì˜ì–´ë¡œ, ëª¨ë“  ê°’(ë‚´ìš©)ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ì˜ˆì‹œ: 'key_concepts', 'estimated_hours' ê°™ì€ í‚¤ëŠ” ì˜ì–´ë¥¼ ìœ ì§€í•˜ê³ , ê·¸ ê°’ë“¤ë§Œ í•œêµ­ì–´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤."),
            HumanMessage(content=prompt)
        ]
        
        print("ğŸ¤– Generating curriculum with LLM...")
        response = await llm.agenerate([messages])
        
        if response.generations and response.generations[0]:
            response_text = response.generations[0][0].text
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                return json.loads(json_match.group())
    
    except Exception as e:
        print(f"âŒ LLM generation failed: {e}")
    
    return create_basic_curriculum(topic, level, duration_weeks)

# ê¸°ë³¸ ì»¤ë¦¬í˜ëŸ¼ ìƒì„± (LLM ì‹¤íŒ¨ì‹œ fallback)
def create_basic_curriculum(topic: str, level: str, duration_weeks: int) -> Dict[str, Any]:
    modules = []
    
    for i in range(1, duration_weeks + 1):
        modules.append({
            "week": i,
            "title": f"{topic} - {i}ì£¼ì°¨",
            "description": f"{i}ì£¼ì°¨ í•™ìŠµ ë‚´ìš©",
            "objectives": [f"{i}ì£¼ì°¨ í•µì‹¬ ê°œë… í•™ìŠµ", "ì‹¤ìŠµ ê³¼ì œ ì™„ë£Œ", "ì´ë¡  ì´í•´ ë° ì ìš©"],
            "key_concepts": [f"{i}ì£¼ì°¨ ê¸°ì´ˆ ê°œë…", "ì‹¤ìŠµ ì˜ˆì œ"],
            "estimated_hours": 8 + i * 2
        })
    
    return {
        "modules": modules,
        "overall_goal": f"{duration_weeks}ì£¼ ë™ì•ˆ {topic} ê¸°ì´ˆë¥¼ ë§ˆìŠ¤í„°í•˜ê¸°"
    }

# === MCP Tools ===

@mcp.tool()
async def list_session_topics() -> Dict[str, Any]:
    """List all completed sessions available for curriculum generation"""
    sessions = session_loader.get_completed_sessions()
    
    if not sessions:
        return {"message": "No completed sessions found", "sessions": []}
    
    topics_count = {}
    for session in sessions:
        topic = session["topic"]
        topics_count[topic] = topics_count.get(topic, 0) + 1
    
    return {
        "total_sessions": len(sessions),
        "unique_topics": len(topics_count),
        "topics": topics_count,
        "all_sessions": sessions
    }

@mcp.tool()
async def generate_curriculum_from_session(session_id: str) -> Dict[str, Any]:
    """Generate curriculum from a specific session"""
    sessions = session_loader.get_completed_sessions()
    session_data = None
    
    for session in sessions:
        if session["session_id"] == session_id:
            session_data = session
            break
    
    if not session_data:
        return {"error": f"Session {session_id} not found"}
    
    # íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    topic = session_data["topic"]
    constraints = session_data["constraints"]
    goal = session_data["goal"]
    
    params = await session_loader.extract_parameters_with_llm(constraints, goal)
    
    print(f"ğŸ“š Generating curriculum for {session_id}:")
    print(f"  Topic: {topic}")
    print(f"  Level: {params['level']}")
    print(f"  Duration: {params['duration_weeks']} weeks")
    
    # í•™ìŠµ ìë£Œ ê²€ìƒ‰
    resources = await search_resources(topic)
    
    # ì»¤ë¦¬í˜ëŸ¼ ìƒì„±
    curriculum_data = await generate_with_llm(
        topic=topic,
        level=params["level"],
        duration_weeks=params["duration_weeks"],
        focus_areas=params["focus_areas"],
        resources=resources
    )
    
    # ìµœì¢… ì»¤ë¦¬í˜ëŸ¼ êµ¬ì„±
    curriculum = {
        "title": f"{topic} Learning Path",
        "level": params["level"],
        "duration_weeks": params["duration_weeks"],
        "modules": curriculum_data.get("modules", []),
        "overall_goal": curriculum_data.get("overall_goal", f"Master {topic}"),
        "resources": resources[:5] if resources else [],  # ìƒìœ„ 5ê°œ ìë£Œë§Œ (ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸)
        "session_id": session_id,
        "original_constraints": constraints,
        "original_goal": goal,
        "generated_at": datetime.now().isoformat()
    }
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
    curriculum_id = db.save_curriculum(session_id, curriculum)
    curriculum["curriculum_id"] = curriculum_id
    
    # ì„¸ì…˜ íŒŒì¼ì—ë„ ì»¤ë¦¬í˜ëŸ¼ ì •ë³´ ì—…ë°ì´íŠ¸
    session_loader.update_session_with_curriculum(session_id, curriculum)
    
    return curriculum

@mcp.tool()
async def generate_curriculums_from_all_sessions() -> Dict[str, Any]:
    """Generate curriculums for all completed sessions"""
    sessions = session_loader.get_completed_sessions()
    
    if not sessions:
        return {
            "message": "No completed sessions found",
            "sessions_processed": 0,
            "curriculums_generated": 0
        }
    
    successful = []
    failed = []
    
    for session in sessions:
        session_id = session["session_id"]
        
        try:
            # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            existing = db.get_curriculum(session_id, 0)
            if existing:
                print(f"âš ï¸ Curriculum already exists for {session_id}")
                continue
            
            result = await generate_curriculum_from_session(session_id)
            
            if "error" in result:
                failed.append({"session_id": session_id, "error": result["error"]})
            else:
                successful.append({
                    "session_id": session_id,
                    "topic": session["topic"],
                    "curriculum_id": result.get("curriculum_id")
                })
                print(f"âœ… Generated curriculum for {session_id}")
                
        except Exception as e:
            failed.append({"session_id": session_id, "error": str(e)})
    
    return {
        "sessions_processed": len(sessions),
        "curriculums_generated": len(successful),
        "successful_generations": successful,
        "failed_generations": failed,
        "success_rate": f"{(len(successful) / len(sessions)) * 100:.1f}%" if sessions else "0%"
    }

@mcp.tool()
async def get_curriculum(user_id: str, curriculum_id: int = 0) -> Dict[str, Any]:
    """Get a specific curriculum"""
    curriculum = db.get_curriculum(user_id, curriculum_id)
    
    if not curriculum:
        return {"error": "Curriculum not found"}
    
    return curriculum

@mcp.tool()
async def search_learning_resources(topic: str, num_results: int = 10) -> Dict[str, Any]:
    """Search for learning resources on a topic"""
    resources = await search_resources(topic, num_results)
    
    return {
        "topic": topic,
        "total_resources": len(resources),
        "resources": resources
    }

if __name__ == "__main__":
    mcp.run(transport="stdio")