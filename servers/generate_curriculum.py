from mcp.server.fastmcp import FastMCP
import asyncio
import httpx
from typing import List, Dict, Any, Optional
from datetime import datetime
import json
import re
from urllib.parse import quote
import os
import glob
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from pydantic import BaseModel, Field
from enum import Enum
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import Config

# MCP ì„œë²„ ì„¤ì •
mcp = FastMCP(
    "CurriculumGenerator",
    instructions="Generate personalized learning curriculums from session data",
    host="0.0.0.0",
    port=8006,
)

# LLM ì´ˆê¸°í™”
def initialize_llm():
    try:
        llm = ChatOpenAI(
            base_url=Config.LLM_BASE_URL,
            api_key=Config.LLM_API_KEY,
            model=Config.LLM_MODEL,
            temperature=0.7,
            max_tokens=Config.LLM_MAX_TOKENS,
            model_kwargs={"max_completion_tokens": None}  # Friendli.aiì—ì„œ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„° ì œê±°
        )
        # print(f"ğŸ¤– LLM initialized: {Config.LLM_MODEL}")  # MCP í†µì‹  ë°©í•´ ë°©ì§€
        return llm, True
    except Exception as e:
        # print(f"âŒ LLM initialization failed: {e}")  # MCP í†µì‹  ë°©í•´ ë°©ì§€
        return None, False

llm, llm_available = initialize_llm()

# Pydantic ëª¨ë¸ ì •ì˜
class LevelEnum(str, Enum):
    BEGINNER = "beginner"
    INTERMEDIATE = "intermediate"
    ADVANCED = "advanced"

class SessionParameters(BaseModel):
    """ì„¸ì…˜ íŒŒë¼ë¯¸í„° ì¶”ì¶œì„ ìœ„í•œ êµ¬ì¡°í™”ëœ ì¶œë ¥"""
    level: LevelEnum = Field(description="í•™ìŠµìì˜ ë ˆë²¨ (beginner/intermediate/advanced)")
    duration_weeks: int = Field(description="í•™ìŠµ ê¸°ê°„ (ì£¼ ë‹¨ìœ„, 1-52 ì‚¬ì´)", ge=1, le=52)  # 1ë…„ê¹Œì§€ í™•ì¥
    focus_areas: List[str] = Field(description="í•™ìŠµ í¬ì»¤ìŠ¤ ì˜ì—­ë“¤")
    
class ExtractionRequest(BaseModel):
    """íŒŒë¼ë¯¸í„° ì¶”ì¶œ ìš”ì²­"""
    constraints: str = Field(description="í•™ìŠµ ì œì•½ ì¡°ê±´")
    goal: str = Field(description="í•™ìŠµ ëª©í‘œ")

# ë°ì´í„°ë² ì´ìŠ¤ í´ë˜ìŠ¤
class CurriculumDB:
    def __init__(self, data_dir: str = "data"):
        self.curriculums = {}
        self.data_dir = data_dir
        os.makedirs(data_dir, exist_ok=True)
        self._load_data()
    
    def _load_data(self):
        try:
            file_path = os.path.join(self.data_dir, "curriculums.json")
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    self.curriculums = json.load(f)
                # print(f"ğŸ“š Loaded {len(self.curriculums)} curriculums")  # MCP í†µì‹  ë°©í•´ ë°©ì§€
        except Exception as e:
            # print(f"âŒ Error loading data: {e}")  # MCP í†µì‹  ë°©í•´ ë°©ì§€
            self.curriculums = {}
            pass
    
    def _save_data(self):
        try:
            file_path = os.path.join(self.data_dir, "curriculums.json")
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(self.curriculums, f, indent=2, ensure_ascii=False)
        except Exception as e:
            # print(f"âŒ Error saving data: {e}")  # MCP í†µì‹  ë°©í•´ ë°©ì§€
            pass
    
    def save_curriculum(self, user_id: str, curriculum: Dict):
        if user_id not in self.curriculums:
            self.curriculums[user_id] = []
        curriculum['id'] = len(self.curriculums[user_id])
        curriculum['created_at'] = datetime.now().isoformat()
        self.curriculums[user_id].append(curriculum)
        self._save_data()
        return curriculum['id']
    
    def get_curriculum(self, user_id: str, curriculum_id: int) -> Optional[Dict]:
        if user_id in self.curriculums and curriculum_id < len(self.curriculums[user_id]):
            return self.curriculums[user_id][curriculum_id]
        return None

db = CurriculumDB()

# ì„¸ì…˜ ë°ì´í„° ë¡œë”
class SessionLoader:
    def __init__(self, sessions_dir: str = "sessions"):
        self.sessions_dir = sessions_dir
    
    def get_completed_sessions(self) -> List[Dict[str, Any]]:
        completed_sessions = []
        session_files = glob.glob(os.path.join(self.sessions_dir, "*.json"))
        
        for session_file in session_files:
            try:
                with open(session_file, 'r', encoding='utf-8') as f:
                    session_data = json.load(f)
                
                if (session_data.get("completed") and 
                    session_data.get("topic") and 
                    session_data.get("constraints") and 
                    session_data.get("goal")):
                    
                    completed_sessions.append({
                        "session_id": session_data.get("session_id"),
                        "topic": session_data.get("topic"),
                        "constraints": session_data.get("constraints"),
                        "goal": session_data.get("goal")
                    })
            except Exception as e:
                # print(f"âš ï¸ Error loading {session_file}: {e}")  # MCP í†µì‹  ë°©í•´ ë°©ì§€
                continue
        
        return completed_sessions
    
    async def extract_parameters_with_llm(self, constraints: str, goal: str, max_retries: int = 3) -> Dict[str, Any]:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì„¸ì…˜ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ í¬í•¨)"""
        if not llm_available:
            # print("âš ï¸ LLM not available, using fallback method")  # MCP í†µì‹  ë°©í•´ ë°©ì§€
            return self.parse_constraints_fallback(constraints, goal)
        
        for attempt in range(max_retries):
            try:
                prompt = f"""ë‹¤ìŒ í•œêµ­ì–´ í•™ìŠµ ì œì•½ì¡°ê±´ê³¼ ëª©í‘œì—ì„œ í•™ìŠµ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

ì œì•½ì¡°ê±´: "{constraints}"
ëª©í‘œ: "{goal}"

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

1. ë ˆë²¨ (level):
   - "ì´ˆë³´", "ì²˜ìŒ", "ì‹œì‘", "beginner" â†’ "beginner"
   - "ì¤‘ê¸‰", "ì–´ëŠì •ë„", "ì¤‘ê°„", "intermediate" â†’ "intermediate"  
   - "ê³ ê¸‰", "ì „ë¬¸", "ìˆ™ë ¨", "advanced" â†’ "advanced"

2. ê¸°ê°„ (duration_weeks):
   - "1ì£¼", "ì¼ì£¼ì¼", "1week" â†’ 1
   - "2ì£¼", "2week", "2ì¼" â†’ 2
   - "1ë‹¬", "í•œë‹¬", "1month", "4ì£¼" â†’ 4
   - "2ë‹¬", "ë‘ë‹¬", "2month", "8ì£¼" â†’ 8
   - "3ë‹¬", "ì„¸ë‹¬", "3month", "12ì£¼" â†’ 12
   - "4ê°œì›”", "4month", "16ì£¼" â†’ 16
   - "5ê°œì›”", "5month", "20ì£¼" â†’ 20
   - "6ê°œì›”", "ë°˜ë…„", "6month", "24ì£¼" â†’ 24
   - "9ê°œì›”", "9month" â†’ 36
   - "1ë…„", "12ê°œì›”", "1year", "52ì£¼" â†’ 52
   - ëª…ì‹œë˜ì§€ ì•Šìœ¼ë©´ â†’ 4

3. í¬ì»¤ìŠ¤ ì˜ì—­ (focus_areas):
   - "ì›¹", "web" â†’ ["web development"]
   - "ë°ì´í„°", "data" â†’ ["data analysis"]
   - "ë¨¸ì‹ ëŸ¬ë‹", "AI", "ì¸ê³µì§€ëŠ¥" â†’ ["machine learning"]
   - "ì•±", "ëª¨ë°”ì¼", "app" â†’ ["mobile development"]
   - "ê°œì¸ í”„ë¡œì íŠ¸", "í† ì´ í”„ë¡œì íŠ¸" â†’ ["personal projects"]
   - "ì·¨ì—…", "ë©´ì ‘", "job" â†’ ["job preparation"]
   - ê¸°íƒ€ ê´€ë ¨ í‚¤ì›Œë“œë“¤ì„ í¬í•¨

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
                
                # Pydanticì„ ì‚¬ìš©í•œ êµ¬ì¡°í™”ëœ ì¶œë ¥
                structured_llm = llm.with_structured_output(SessionParameters)
                
                messages = [
                    SystemMessage(content="ë‹¹ì‹ ì€ í•œêµ­ì–´ í•™ìŠµ ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
                    HumanMessage(content=prompt)
                ]
                
                # print(f"ğŸ¤– LLM parameter extraction attempt {attempt + 1}/{max_retries}...")  # MCP í†µì‹  ë°©í•´ ë°©ì§€
                result = await structured_llm.ainvoke(messages)
                
                # ì„±ê³µí•˜ë©´ ê²°ê³¼ ë°˜í™˜
                extracted_params = {
                    "level": result.level.value,
                    "duration_weeks": result.duration_weeks,
                    "focus_areas": result.focus_areas
                }
                # print(f"âœ… LLM extraction successful on attempt {attempt + 1}")  # MCP í†µì‹  ë°©í•´ ë°©ì§€
                return extracted_params
            
            except Exception as e:
                # print(f"âš ï¸ LLM extraction attempt {attempt + 1} failed: {e}")  # MCP í†µì‹  ë°©í•´ ë°©ì§€
                
                # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ê³„ì† ì¬ì‹œë„
                if attempt < max_retries - 1:
                    # print(f"ğŸ”„ Retrying... ({max_retries - attempt - 1} attempts remaining)")  # MCP í†µì‹  ë°©í•´ ë°©ì§€
                    continue
                else:
                    # print(f"âŒ All {max_retries} LLM attempts failed, falling back to rule-based parsing")  # MCP í†µì‹  ë°©í•´ ë°©ì§€
                    break
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ì‹œ fallback ì‚¬ìš©
        return self.parse_constraints_fallback(constraints, goal)
    
    def parse_constraints_fallback(self, constraints: str, goal: str) -> Dict[str, Any]:
        """LLM ì‹¤íŒ¨ì‹œ ê¸°ì¡´ ê·œì¹™ ê¸°ë°˜ íŒŒì‹± (fallback)"""
        constraints_lower = constraints.lower()
        goal_lower = goal.lower()
        
        # ë ˆë²¨ íŒŒì‹±
        level = "beginner"
        if any(word in constraints_lower for word in ["ì¤‘ê¸‰", "ì–´ëŠì •ë„", "intermediate"]):
            level = "intermediate"
        elif any(word in constraints_lower for word in ["ê³ ê¸‰", "ì „ë¬¸", "advanced"]):
            level = "advanced"
        
        # ê¸°ê°„ íŒŒì‹± (ì£¼ ë‹¨ìœ„) - í™•ì¥ëœ ë²„ì „
        duration_weeks = 4  # ê¸°ë³¸ê°’
        if any(word in constraints_lower for word in ["1ì£¼", "1week"]):
            duration_weeks = 1
        elif any(word in constraints_lower for word in ["2ì£¼", "2week", "2ì¼"]):
            duration_weeks = 2
        elif any(word in constraints_lower for word in ["1ë‹¬", "1month", "4ì£¼"]):
            duration_weeks = 4
        elif any(word in constraints_lower for word in ["2ë‹¬", "2month", "8ì£¼"]):
            duration_weeks = 8
        elif any(word in constraints_lower for word in ["3ë‹¬", "3month", "12ì£¼"]):
            duration_weeks = 12
        elif any(word in constraints_lower for word in ["4ê°œì›”", "4month", "16ì£¼"]):
            duration_weeks = 16
        elif any(word in constraints_lower for word in ["5ê°œì›”", "5month", "20ì£¼"]):
            duration_weeks = 20
        elif any(word in constraints_lower for word in ["6ê°œì›”", "ë°˜ë…„", "6month", "24ì£¼"]):
            duration_weeks = 24
        elif any(word in constraints_lower for word in ["9ê°œì›”", "9month"]):
            duration_weeks = 36
        elif any(word in constraints_lower for word in ["1ë…„", "12ê°œì›”", "1year", "52ì£¼"]):
            duration_weeks = 52
        
        # í¬ì»¤ìŠ¤ ì˜ì—­ ì¶”ì¶œ
        focus_areas = []
        mappings = {
            "ì›¹": ["web development"],
            "ë°ì´í„°": ["data analysis"],
            "ë¨¸ì‹ ëŸ¬ë‹": ["machine learning"],
            "ì•±": ["mobile development"],
            "ê°œì¸ í”„ë¡œì íŠ¸": ["personal projects"],
            "ì·¨ì—…": ["job preparation"]
        }
        
        for keyword, areas in mappings.items():
            if keyword in goal_lower:
                focus_areas.extend(areas)
        
        return {
            "level": level, 
            "duration_weeks": duration_weeks,
            "focus_areas": list(set(focus_areas))
        }
    
    def update_session_with_curriculum(self, session_id: str, curriculum: Dict[str, Any]) -> bool:
        """ì„¸ì…˜ JSON íŒŒì¼ì— ì»¤ë¦¬í˜ëŸ¼ ì •ë³´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤"""
        try:
            session_file_path = os.path.join(self.sessions_dir, f"{session_id}.json")
            
            if not os.path.exists(session_file_path):
                # print(f"âš ï¸ Session file not found: {session_file_path}")  # MCP í†µì‹  ë°©í•´ ë°©ì§€
                return False
            
            # ê¸°ì¡´ ì„¸ì…˜ ë°ì´í„° ë¡œë“œ
            with open(session_file_path, 'r', encoding='utf-8') as f:
                session_data = json.load(f)
            
            # ëª¨ë“ˆ ë°ì´í„° ì •ê·œí™” (LLM ì˜¤ë¥˜ ìˆ˜ì •)
            modules = curriculum.get("modules", [])
            cleaned_modules = []
            
            for module in modules:
                cleaned_module = {}
                for key, value in module.items():
                    # í‚¤ ì •ê·œí™”: í•œêµ­ì–´ í‚¤ë¥¼ ì˜ì–´ë¡œ ë§¤í•‘
                    cleaned_key = key.replace("_ ", "_").replace(" _", "_").strip()
                    
                    # í•œêµ­ì–´ í‚¤ ë§¤í•‘
                    key_mapping = {
                        "key_ê°œë…ë“¤": "key_concepts",
                        "key_ê°œë…": "key_concepts",
                        "estimated_ì‹œê°„": "estimated_hours",
                        "estimated_ì‹œê°„": "estimated_hours",
                        "í•™ìŠµëª©í‘œ": "objectives",
                        "ì œëª©": "title",
                        "ì„¤ëª…": "description",
                        "ì£¼ì°¨": "week"
                    }
                    
                    if cleaned_key in key_mapping:
                        cleaned_key = key_mapping[cleaned_key]
                    
                    cleaned_module[cleaned_key] = value
                cleaned_modules.append(cleaned_module)
            
            # ì„¸ë¶€ ì»¤ë¦¬í˜ëŸ¼ ì •ë³´ ì¶”ê°€ (ì „ì²´ ë‚´ìš© í¬í•¨)
            curriculum_full = {
                "curriculum_generated": True,
                "curriculum_id": curriculum.get("curriculum_id"),
                "title": curriculum.get("title"),
                "level": curriculum.get("level"),
                "duration_weeks": curriculum.get("duration_weeks"),
                "overall_goal": curriculum.get("overall_goal"),
                "generated_at": curriculum.get("generated_at"),
                "original_constraints": curriculum.get("original_constraints"),
                "original_goal": curriculum.get("original_goal"),
                
                # ì •ê·œí™”ëœ ëª¨ë“ˆ ì •ë³´ í¬í•¨
                "modules": cleaned_modules,
                "modules_count": len(cleaned_modules),
                
                # í•™ìŠµ ìë£Œ ì •ë³´ í¬í•¨
                "resources": curriculum.get("resources", []),
                "resources_count": len(curriculum.get("resources", [])),
                
                # í†µê³„ ì •ë³´ ê³„ì‚°
                "total_estimated_hours": sum(
                    module.get("estimated_hours", module.get("estimated_ hours", 0)) 
                    for module in cleaned_modules
                ),
                "average_hours_per_week": 0  # ì„ì‹œë¡œ 0ìœ¼ë¡œ ì„¤ì •
            }
            
            # í‰ê·  ì‹œê°„ ì¬ê³„ì‚°
            total_hours = curriculum_full["total_estimated_hours"]
            duration = curriculum.get("duration_weeks", 1)
            curriculum_full["average_hours_per_week"] = (
                total_hours / duration
            ) if duration > 0 else 0
            
            session_data["curriculum"] = curriculum_full
            
            # íŒŒì¼ì— ë‹¤ì‹œ ì €ì¥
            with open(session_file_path, 'w', encoding='utf-8') as f:
                json.dump(session_data, f, indent=2, ensure_ascii=False)
            
            # print(f"âœ… Session {session_id} updated with curriculum info")  # MCP í†µì‹  ë°©í•´ ë°©ì§€
            return True
            
        except Exception as e:
            # print(f"âŒ Failed to update session {session_id}: {e}")  # MCP í†µì‹  ë°©í•´ ë°©ì§€
            return False

session_loader = SessionLoader()

# ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ê¸°ê°„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def extract_duration_from_message(message: str) -> int:
    """ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ê¸°ê°„ì„ ì¶”ì¶œí•˜ì—¬ ì£¼ ë‹¨ìœ„ë¡œ ë°˜í™˜"""
    message_lower = message.lower()
    
    # ê¸°ê°„ í‚¤ì›Œë“œ ë§¤í•‘ (ì£¼ ë‹¨ìœ„)
    duration_patterns = {
        "1ì£¼": 1, "1week": 1, "ì¼ì£¼ì¼": 1,
        "2ì£¼": 2, "2week": 2, "ì´ì£¼": 2,
        "1ê°œì›”": 4, "1month": 4, "í•œë‹¬": 4, "4ì£¼": 4,
        "2ê°œì›”": 8, "2month": 8, "ë‘ë‹¬": 8, "8ì£¼": 8,
        "3ê°œì›”": 12, "3month": 12, "ì„¸ë‹¬": 12, "12ì£¼": 12,
        "4ê°œì›”": 16, "4month": 16, "16ì£¼": 16,
        "5ê°œì›”": 20, "5month": 20, "20ì£¼": 20,
        "6ê°œì›”": 24, "6month": 24, "ë°˜ë…„": 24, "24ì£¼": 24,
        "9ê°œì›”": 36, "9month": 36,
        "1ë…„": 52, "12ê°œì›”": 52, "1year": 52, "52ì£¼": 52
    }
    
    # ë©”ì‹œì§€ì—ì„œ ê¸°ê°„ í‚¤ì›Œë“œ ì°¾ê¸°
    for keyword, weeks in duration_patterns.items():
        if keyword in message_lower:
            return weeks
    
    return None  # ê¸°ê°„ ì •ë³´ê°€ ì—†ìœ¼ë©´ None ë°˜í™˜

# K-MOOC Summary íŒŒì‹± í—¬í¼ í•¨ìˆ˜
def parse_kmooc_summary(summary: str) -> Dict[str, str]:
    """K-MOOC summaryì—ì„œ ê°•ì¢Œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤"""
    try:
        if not summary:
            return {}
        
        parsed_info = {}
        
        # ê°•ì¢Œ ëª©í‘œ ì¶”ì¶œ
        goal_match = re.search(r'\*\*ê°•ì¢Œ ëª©í‘œ:\*\*\s*([^\n*]+)', summary)
        if goal_match:
            parsed_info["course_goal"] = goal_match.group(1).strip()
            # ê°•ì¢Œ ëª©í‘œì—ì„œ ì²« ë²ˆì§¸ ë¬¸ì¥ì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš©
            goal_text = goal_match.group(1).strip()
            # ì²« ë²ˆì§¸ ë¬¸ì¥ì´ë‚˜ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì œëª©ìœ¼ë¡œ ì¶”ì¶œ
            if "," in goal_text:
                parsed_info["title"] = goal_text.split(",")[0].strip()
            else:
                parsed_info["title"] = goal_text[:50] + "..." if len(goal_text) > 50 else goal_text
        
        # ì£¼ìš” ë‚´ìš© ì¶”ì¶œ
        content_match = re.search(r'\*\*ì£¼ìš” ë‚´ìš©:\*\*\s*([^\n*]+)', summary)
        if content_match:
            content = content_match.group(1).strip()
            parsed_info["main_content"] = content
            # ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ ì„¤ëª…ìœ¼ë¡œ ì‚¬ìš©
            if len(content) > 100:
                parsed_info["description"] = content[:97] + "..."
            else:
                parsed_info["description"] = content
        
        # ê°•ì¢Œ ê¸°ê°„ ì¶”ì¶œ
        duration_match = re.search(r'\*\*ê°•ì¢Œ ê¸°ê°„:\*\*[^()]*\((\d+ì£¼)\)', summary)
        if duration_match:
            parsed_info["duration"] = duration_match.group(1)
        
        # ë‚œì´ë„ ì¶”ì¶œ
        difficulty_match = re.search(r'\*\*ë‚œì´ë„:\*\*\s*([^\n*]+)', summary)
        if difficulty_match:
            parsed_info["difficulty"] = difficulty_match.group(1).strip()
        
        # ìˆ˜ì—… ì‹œê°„ ì¶”ì¶œ
        time_match = re.search(r'\*\*ìˆ˜ì—… ì‹œê°„:\*\*[^()]*ì•½\s*([^\n*()]+)', summary)
        if time_match:
            parsed_info["class_time"] = time_match.group(1).strip()
        
        print(f"DEBUG: Parsed summary - title: {parsed_info.get('title', 'N/A')}, description: {parsed_info.get('description', 'N/A')[:50]}...", file=sys.stderr, flush=True)
        
        return parsed_info
        
    except Exception as e:
        print(f"DEBUG: Summary parsing failed: {e}", file=sys.stderr, flush=True)
        return {}

# K-MOOC DB ê²€ìƒ‰ (Pinecone API ì—°ë™)
async def search_kmooc_resources(topic: str, week_title: str = None, top_k: int = 5) -> List[Dict[str, Any]]:
    """K-MOOC DBì—ì„œ ê´€ë ¨ ì˜ìƒì„ ê²€ìƒ‰í•©ë‹ˆë‹¤"""
    try:
        # Pinecone ê²€ìƒ‰ API í˜¸ì¶œ
        search_query = f"{topic}"
        if week_title:
            search_query += f" {week_title}"
            
        search_payload = {
            "query": search_query,
            "top_k": top_k,
            "namespace": "kmooc_engineering",
            "filter": {"institution": {"$ne": ""}},
            "rerank": True,
            "include_metadata": True
        }
        
        # pinecone_use.py ì„œë²„ê°€ localhost:8000ì—ì„œ ì‹¤í–‰ ì¤‘ì´ë¼ê³  ê°€ì •
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "http://localhost:8001/search",
                json=search_payload,
                timeout=10.0
            )
            
        if response.status_code == 200:
            result = response.json()
            kmooc_videos = []
            
            for item in result.get("results", []):
                metadata = item.get("metadata", {})
                if metadata:
                    # Summary íŒŒì‹±í•˜ì—¬ ê°•ì¢Œ ì •ë³´ ì¶”ì¶œ
                    summary = metadata.get("summary", "")
                    parsed_info = parse_kmooc_summary(summary)
                    
                    # ì œëª© ê²°ì •: íŒŒì‹±ëœ ì œëª© > ê¸°ë³¸ "K-MOOC ê°•ì¢Œ"
                    course_title = parsed_info.get("title") or "K-MOOC ê°•ì¢Œ"
                    
                    # ì„¤ëª… ê²°ì •: íŒŒì‹±ëœ ì„¤ëª… > ì£¼ìš” ë‚´ìš© > ê°•ì¢Œ ëª©í‘œ > ê¸°ë³¸ ë©”ì‹œì§€
                    description = (
                        parsed_info.get("description") or 
                        parsed_info.get("main_content") or 
                        parsed_info.get("course_goal") or 
                        "K-MOOC ì˜¨ë¼ì¸ ê°•ì¢Œ"
                    )
                    
                    video_info = {
                        "title": course_title,
                        "description": description,
                        "url": metadata.get("url", ""),
                        "institution": metadata.get("institution", "").replace(" ìš´ì˜ê¸°ê´€ ë°”ë¡œê°€ê¸°ìƒˆì°½ì—´ë¦¼", ""),
                        "course_goal": parsed_info.get("course_goal", ""),
                        "duration": parsed_info.get("duration", ""),
                        "difficulty": parsed_info.get("difficulty", ""),
                        "class_time": parsed_info.get("class_time", ""),
                        "score": item.get("score", 0.0),
                        "source": "K-MOOC"
                    }
                    kmooc_videos.append(video_info)
            
            return kmooc_videos
            
    except Exception as e:
        print(f"DEBUG: K-MOOC search failed: {e}", file=sys.stderr, flush=True)
        pass
    
    return []

# í•™ìŠµ ìë£Œ ê²€ìƒ‰ (ì›¹ ê²€ìƒ‰)
async def search_resources(topic: str, num_results: int = 10) -> List[Dict[str, str]]:
    try:
        encoded_query = quote(f"{topic} tutorial")
        url = f"https://lite.duckduckgo.com/lite/?q={encoded_query}"
        
        async with httpx.AsyncClient() as client:
            response = await client.get(url, timeout=10.0)
            
            if response.status_code == 200:
                content = response.text
                results = []
                
                link_pattern = r'<a[^>]*class="result-link"[^>]*href="([^"]*)"[^>]*>(.*?)</a>'
                matches = re.finditer(link_pattern, content)
                
                for i, match in enumerate(matches):
                    if len(results) >= num_results:
                        break
                    results.append({
                        "title": re.sub(r'<[^>]+>', '', match.group(2)).strip(),
                        "url": match.group(1),
                        "source": "Web Search"
                    })
                
                return results
    except Exception as e:
        # print(f"âŒ Search failed: {e}")  # MCP í†µì‹  ë°©í•´ ë°©ì§€
        pass
    
    return []

# LLMì„ ì‚¬ìš©í•œ ì»¤ë¦¬í˜ëŸ¼ ìƒì„±
async def generate_with_llm(topic: str, level: str, duration_weeks: int, focus_areas: List[str], resources: List[Dict[str, str]] = None) -> Dict[str, Any]:
    if not llm_available:
        return create_basic_curriculum(topic, level, duration_weeks)
    
    try:
        print(f"DEBUG: generate_with_llm called - topic:{topic}, level:{level}, duration:{duration_weeks}", file=sys.stderr, flush=True)
        
        focus_text = ', '.join(focus_areas) if focus_areas else 'General coverage'
        print(f"DEBUG: Focus areas processed: {focus_text}", file=sys.stderr, flush=True)
        
        # í•™ìŠµ ìë£Œê°€ ìˆìœ¼ë©´ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
        resources_text = ""
        if resources and len(resources) > 0:
            print(f"DEBUG: Processing {len(resources)} resources for prompt", file=sys.stderr, flush=True)
            resources_text = "\n\nAvailable learning resources:\n"
            for i, resource in enumerate(resources[:5], 1):
                resources_text += f"{i}. {resource.get('title', 'No title')} - {resource.get('url', 'No URL')}\n"
            resources_text += "\nConsider these resources when designing the curriculum modules.\n"
        else:
            print(f"DEBUG: No resources found, using fallback text", file=sys.stderr, flush=True)
            resources_text = "\n\nNote: No specific learning resources were found, but design a comprehensive curriculum anyway.\n"
        
        prompt = f"""ë‹¤ìŒ ì¡°ê±´ì— ë§ëŠ” {duration_weeks}ì£¼ ì»¤ë¦¬í˜ëŸ¼ì„ ìƒì„±í•´ì£¼ì„¸ìš”:

í•™ìŠµ ì£¼ì œ: {topic}
í•™ìŠµ ë ˆë²¨: {level}
í¬ì»¤ìŠ¤ ì˜ì—­: {focus_text}{resources_text}
ì¤‘ìš”: JSON í‚¤ëŠ” ì˜ì–´ë¡œ, ëª¨ë“  ë‚´ìš©ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”!

ê° ëª¨ë“ˆì€ ë‹¤ìŒì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:
- ëª…í™•í•œ ì œëª©ê³¼ ì„¤ëª… (í•œêµ­ì–´)
- 3-4ê°œì˜ í•™ìŠµ ëª©í‘œ (í•œêµ­ì–´)
- í•™ìŠµ ì„±ê³¼ ("ë‚´ê°€ ë°°ìš¸ ìˆ˜ ìˆëŠ” ê²ƒ") (í•œêµ­ì–´)
- ì˜ˆìƒ í•™ìŠµ ì‹œê°„
- í•µì‹¬ ê°œë…ë“¤ (í•œêµ­ì–´)

JSON í˜•ì‹ (í‚¤ëŠ” ì˜ì–´, ê°’ì€ í•œêµ­ì–´):
{{
    "modules": [
        {{
            "week": 1,
            "title": "ëª¨ë“ˆ ì œëª© (í•œêµ­ì–´)",
            "description": "ëª¨ë“ˆì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª… (í•œêµ­ì–´)",
            "objectives": ["í•™ìŠµëª©í‘œ1 (í•œêµ­ì–´)", "í•™ìŠµëª©í‘œ2 (í•œêµ­ì–´)", "í•™ìŠµëª©í‘œ3 (í•œêµ­ì–´)"],
            "learning_outcomes": ["ë‚´ê°€ ë°°ìš¸ ìˆ˜ ìˆëŠ” ê²ƒ1 (í•œêµ­ì–´)", "ë‚´ê°€ ë°°ìš¸ ìˆ˜ ìˆëŠ” ê²ƒ2 (í•œêµ­ì–´)"],
            "key_concepts": ["í•µì‹¬ê°œë… 1 (í•œêµ­ì–´)", "í•µì‹¬ê°œë… 2 (í•œêµ­ì–´)"],
            "estimated_hours": 10
        }}
    ],
    "overall_goal": "ì „ì²´ í•™ìŠµ ëª©í‘œ (í•œêµ­ì–´)"
}}"""
        
        print(f"DEBUG: Prompt constructed. Length: {len(prompt)} chars", file=sys.stderr, flush=True)
        
        messages = [
            SystemMessage(content="ë‹¹ì‹ ì€ ì „ë¬¸ ì»¤ë¦¬í˜ëŸ¼ ì„¤ê³„ìì…ë‹ˆë‹¤. ë°˜ë“œì‹œ JSON í‚¤ëŠ” ì˜ì–´ë¡œ, ëª¨ë“  ê°’(ë‚´ìš©)ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ì˜ˆì‹œ: 'key_concepts', 'estimated_hours' ê°™ì€ í‚¤ëŠ” ì˜ì–´ë¥¼ ìœ ì§€í•˜ê³ , ê·¸ ê°’ë“¤ë§Œ í•œêµ­ì–´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤."),
            HumanMessage(content=prompt)
        ]
        
        print(f"DEBUG: Calling LLM.agenerate() - this may take a while for {duration_weeks} weeks...", file=sys.stderr, flush=True)
        import time
        start_time = time.time()
        
        # print("ğŸ¤– Generating curriculum with LLM...")  # MCP í†µì‹  ë°©í•´ ë°©ì§€
        response = await llm.agenerate([messages])
        
        end_time = time.time()
        print(f"DEBUG: LLM.agenerate() completed in {end_time - start_time:.2f} seconds", file=sys.stderr, flush=True)
        
        if response.generations and response.generations[0]:
            response_text = response.generations[0][0].text
            print(f"DEBUG: LLM response received. Length: {len(response_text)} chars", file=sys.stderr, flush=True)
            
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                print(f"DEBUG: JSON found in response. Parsing...", file=sys.stderr, flush=True)
                parsed_json = json.loads(json_match.group())
                print(f"DEBUG: JSON parsed successfully. Modules count: {len(parsed_json.get('modules', []))}", file=sys.stderr, flush=True)
                return parsed_json
            else:
                print(f"DEBUG: No valid JSON found in LLM response", file=sys.stderr, flush=True)
        else:
            print(f"DEBUG: No generations found in LLM response", file=sys.stderr, flush=True)
    
    except Exception as e:
        print(f"DEBUG: Exception in generate_with_llm: {type(e).__name__}: {e}", file=sys.stderr, flush=True)
        # print(f"âŒ LLM generation failed: {e}")  # MCP í†µì‹  ë°©í•´ ë°©ì§€
        pass
    
    return create_basic_curriculum(topic, level, duration_weeks)

# ëª¨ë“ˆë³„ ë¦¬ì†ŒìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜
async def collect_module_resources(topic: str, module_info: Dict[str, Any]) -> Dict[str, List[Dict[str, Any]]]:
    """ì£¼ì°¨ë³„ ëª¨ë“ˆì— ëŒ€í•œ K-MOOC ì˜ìƒê³¼ ì›¹ ë¦¬ì†ŒìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤"""
    try:
        # K-MOOC ê²€ìƒ‰ê³¼ ì›¹ ê²€ìƒ‰ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
        import asyncio
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        week_title = module_info.get('title', '')
        key_concepts = module_info.get('key_concepts', [])
        search_keywords = f"{topic} {week_title}"
        if key_concepts:
            search_keywords += f" {key_concepts[0]}"
        
        # ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
        kmooc_task = search_kmooc_resources(topic, week_title, top_k=3)
        web_task = search_resources(search_keywords, num_results=3)
        
        kmooc_results, web_results = await asyncio.gather(
            kmooc_task, web_task, return_exceptions=True
        )
        
        # ì˜ˆì™¸ ì²˜ë¦¬
        if isinstance(kmooc_results, Exception):
            print(f"DEBUG: K-MOOC search exception: {kmooc_results}", file=sys.stderr, flush=True)
            kmooc_results = []
        if isinstance(web_results, Exception):
            print(f"DEBUG: Web search exception: {web_results}", file=sys.stderr, flush=True)
            web_results = []
        
        return {
            "videos": kmooc_results or [],
            "web_links": web_results or [],
            "documents": []  # í–¥í›„ êµ¬í˜„ ì˜ˆì •
        }
        
    except Exception as e:
        print(f"DEBUG: collect_module_resources failed: {e}", file=sys.stderr, flush=True)
        return {
            "videos": [],
            "web_links": [],
            "documents": []
        }

# ê¸°ë³¸ ì»¤ë¦¬í˜ëŸ¼ ìƒì„± (LLM ì‹¤íŒ¨ì‹œ fallback)
def create_basic_curriculum(topic: str, level: str, duration_weeks: int) -> Dict[str, Any]:
    modules = []
    
    for i in range(1, duration_weeks + 1):
        modules.append({
            "week": i,
            "title": f"{topic} - {i}ì£¼ì°¨",
            "description": f"{i}ì£¼ì°¨ í•™ìŠµ ë‚´ìš©",
            "objectives": [f"{i}ì£¼ì°¨ í•µì‹¬ ê°œë… í•™ìŠµ", "ì‹¤ìŠµ ê³¼ì œ ì™„ë£Œ", "ì´ë¡  ì´í•´ ë° ì ìš©"],
            "learning_outcomes": [f"{topic} ê¸°ë³¸ ê°œë… ì´í•´", f"{i}ì£¼ì°¨ ì‹¤ë¬´ ì§€ì‹ ìŠµë“"],
            "key_concepts": [f"{i}ì£¼ì°¨ ê¸°ì´ˆ ê°œë…", "ì‹¤ìŠµ ì˜ˆì œ"],
            "estimated_hours": 8 + i * 2
        })
    
    return {
        "modules": modules,
        "overall_goal": f"{duration_weeks}ì£¼ ë™ì•ˆ {topic} ê¸°ì´ˆë¥¼ ë§ˆìŠ¤í„°í•˜ê¸°"
    }

# === MCP Tools ===

@mcp.tool()
async def list_session_topics() -> Dict[str, Any]:
    """List all completed sessions available for curriculum generation"""
    sessions = session_loader.get_completed_sessions()
    
    if not sessions:
        return {"message": "No completed sessions found", "sessions": []}
    
    topics_count = {}
    for session in sessions:
        topic = session["topic"]
        topics_count[topic] = topics_count.get(topic, 0) + 1
    
    return {
        "total_sessions": len(sessions),
        "unique_topics": len(topics_count),
        "topics": topics_count,
        "all_sessions": sessions
    }

@mcp.tool()
async def generate_curriculum_from_session(session_id: str, user_message: str = "") -> Dict[str, Any]:
    """Generate curriculum from a specific session with optional user message for duration override"""
    sessions = session_loader.get_completed_sessions()
    session_data = None
    
    for session in sessions:
        if session["session_id"] == session_id:
            session_data = session
            break
    
    if not session_data:
        return {"error": f"Session {session_id} not found"}
    
    # íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    topic = session_data["topic"]
    constraints = session_data["constraints"]
    goal = session_data["goal"]
    
    params = await session_loader.extract_parameters_with_llm(constraints, goal)
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ê¸°ê°„ ì •ë³´ ì¶”ì¶œ ë° ì˜¤ë²„ë¼ì´ë“œ
    if user_message:
        message_duration = extract_duration_from_message(user_message)
        print(f"DEBUG: user_message='{user_message}'", file=sys.stderr, flush=True)
        print(f"DEBUG: extracted duration={message_duration}", file=sys.stderr, flush=True)
        print(f"DEBUG: original params duration={params.get('duration_weeks')}", file=sys.stderr, flush=True)
        
        if message_duration is not None:
            params["duration_weeks"] = message_duration
            print(f"DEBUG: overridden to {message_duration} weeks", file=sys.stderr, flush=True)
        else:
            print(f"DEBUG: no duration found in message", file=sys.stderr, flush=True)
    
    # print(f"ğŸ“š Generating curriculum for {session_id}:")  # MCP í†µì‹  ë°©í•´ ë°©ì§€
    # print(f"  Topic: {topic}")  # MCP í†µì‹  ë°©í•´ ë°©ì§€
    # print(f"  Level: {params['level']}")  # MCP í†µì‹  ë°©í•´ ë°©ì§€
    # print(f"  Duration: {params['duration_weeks']} weeks")  # MCP í†µì‹  ë°©í•´ ë°©ì§€
    
    # ê¸°ë³¸ í•™ìŠµ ìë£Œ ê²€ìƒ‰ (í”„ë¡¬í”„íŠ¸ìš©)
    print(f"DEBUG: Starting basic resource search for topic: {topic}", file=sys.stderr, flush=True)
    basic_resources = await search_resources(topic)
    print(f"DEBUG: Basic resource search completed. Found {len(basic_resources)} resources", file=sys.stderr, flush=True)
    
    # ì»¤ë¦¬í˜ëŸ¼ ìƒì„± (ê¸°ë³¸ êµ¬ì¡°)
    print(f"DEBUG: Starting LLM curriculum generation...", file=sys.stderr, flush=True)
    print(f"DEBUG: LLM parameters - topic:{topic}, level:{params['level']}, duration:{params['duration_weeks']}, focus_areas:{params['focus_areas']}", file=sys.stderr, flush=True)
    print(f"DEBUG: llm_available status: {llm_available}", file=sys.stderr, flush=True)
    
    try:
        curriculum_data = await generate_with_llm(
            topic=topic,
            level=params["level"],
            duration_weeks=params["duration_weeks"],
            focus_areas=params["focus_areas"],
            resources=basic_resources
        )
        print(f"DEBUG: LLM curriculum generation completed successfully", file=sys.stderr, flush=True)
        print(f"DEBUG: Generated {len(curriculum_data.get('modules', []))} modules", file=sys.stderr, flush=True)
    except Exception as e:
        print(f"DEBUG: LLM curriculum generation failed with error: {type(e).__name__}: {e}", file=sys.stderr, flush=True)
        raise
    
    # ê° ëª¨ë“ˆì— ëŒ€í•´ êµ¬ì¡°í™”ëœ ë¦¬ì†ŒìŠ¤ ìˆ˜ì§‘
    modules = curriculum_data.get("modules", [])
    print(f"DEBUG: Processing {len(modules)} modules for resource collection", file=sys.stderr, flush=True)
    
    for module in modules:
        module_topic = f"{topic} {module.get('title', '')}"
        week_title = module.get('title', '')
        
        print(f"DEBUG: Collecting resources for module: {week_title}", file=sys.stderr, flush=True)
        
        # ë³‘ë ¬ë¡œ ë¦¬ì†ŒìŠ¤ ìˆ˜ì§‘ (K-MOOC + ì›¹ ê²€ìƒ‰)
        module_resources = await collect_module_resources(module_topic, module)
        
        # ëª¨ë“ˆì— ë¦¬ì†ŒìŠ¤ ì¶”ê°€
        module["resources"] = {
            "videos": module_resources.get("videos", []),
            "web_links": module_resources.get("web_links", []),
            "documents": []  # ì¶”í›„ ë¬¸ì„œ ê²€ìƒ‰ API ì—°ë™ ì‹œ ì‚¬ìš©
        }
        
        print(f"DEBUG: Added {len(module_resources.get('videos', []))} videos and {len(module_resources.get('web_links', []))} web links", file=sys.stderr, flush=True)
    
    # ìµœì¢… ì»¤ë¦¬í˜ëŸ¼ êµ¬ì„±
    curriculum = {
        "title": f"{topic} Learning Path",
        "level": params["level"],
        "duration_weeks": params["duration_weeks"],
        "modules": modules,  # ë¦¬ì†ŒìŠ¤ê°€ í¬í•¨ëœ ëª¨ë“ˆë“¤
        "overall_goal": curriculum_data.get("overall_goal", f"Master {topic}"),
        "resources": basic_resources[:5] if basic_resources else [],  # ì „ì²´ ì°¸ê³  ìë£Œ
        "session_id": session_id,
        "original_constraints": constraints,
        "original_goal": goal,
        "generated_at": datetime.now().isoformat()
    }
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
    curriculum_id = db.save_curriculum(session_id, curriculum)
    curriculum["curriculum_id"] = curriculum_id
    
    # ì„¸ì…˜ íŒŒì¼ì—ë„ ì»¤ë¦¬í˜ëŸ¼ ì •ë³´ ì—…ë°ì´íŠ¸
    session_loader.update_session_with_curriculum(session_id, curriculum)
    
    return curriculum

@mcp.tool()
async def generate_curriculums_from_all_sessions() -> Dict[str, Any]:
    """Generate curriculums for all completed sessions"""
    sessions = session_loader.get_completed_sessions()
    
    if not sessions:
        return {
            "message": "No completed sessions found",
            "sessions_processed": 0,
            "curriculums_generated": 0
        }
    
    successful = []
    failed = []
    
    for session in sessions:
        session_id = session["session_id"]
        
        try:
            # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            existing = db.get_curriculum(session_id, 0)
            if existing:
                # print(f"âš ï¸ Curriculum already exists for {session_id}")  # MCP í†µì‹  ë°©í•´ ë°©ì§€
                continue
            
            result = await generate_curriculum_from_session(session_id)
            
            if "error" in result:
                failed.append({"session_id": session_id, "error": result["error"]})
            else:
                successful.append({
                    "session_id": session_id,
                    "topic": session["topic"],
                    "curriculum_id": result.get("curriculum_id")
                })
                # print(f"âœ… Generated curriculum for {session_id}")  # MCP í†µì‹  ë°©í•´ ë°©ì§€
                
        except Exception as e:
            failed.append({"session_id": session_id, "error": str(e)})
    
    return {
        "sessions_processed": len(sessions),
        "curriculums_generated": len(successful),
        "successful_generations": successful,
        "failed_generations": failed,
        "success_rate": f"{(len(successful) / len(sessions)) * 100:.1f}%" if sessions else "0%"
    }

@mcp.tool()
async def get_curriculum(user_id: str, curriculum_id: int = 0) -> Dict[str, Any]:
    """Get a specific curriculum"""
    curriculum = db.get_curriculum(user_id, curriculum_id)
    
    if not curriculum:
        return {"error": "Curriculum not found"}
    
    return curriculum

@mcp.tool()
async def search_learning_resources(topic: str, num_results: int = 10) -> Dict[str, Any]:
    """Search for learning resources on a topic"""
    resources = await search_resources(topic, num_results)
    
    return {
        "topic": topic,
        "total_resources": len(resources),
        "resources": resources
    }

if __name__ == "__main__":
    mcp.run(transport="stdio")